<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.1.2//EN"
"http://www.oasis-open.org/docbook/xml/4.1.2/docbookx.dtd">
<book lang="en_US">
  <bookinfo>
    <title>Machine Learning Library Reference</title>

    <mediaobject>
      <imageobject>
        <imagedata fileref="images/redswooshWithLogo3.jpg" />
      </imageobject>
    </mediaobject>

    <author>
      <surname>Boca Raton Documentation Team</surname>
    </author>

    <legalnotice>
      <para>We welcome your comments and feedback about this document via
      email to <email>docfeedback@hpccsystems.com</email> Please include
      <emphasis role="bold">Documentation Feedback</emphasis> in the subject
      line and reference the document name, page numbers, and current Version
      Number in the text of the message.</para>

      <para>LexisNexis and the Knowledge Burst logo are registered trademarks
      of Reed Elsevier Properties Inc., used under license. Other products,
      logos, and services may be trademarks or registered trademarks of their
      respective companies. All names and example data used in this manual are
      fictitious. Any similarity to actual persons, living or dead, is purely
      coincidental.</para>

      <para></para>
    </legalnotice>

    <releaseinfo>© 2012 HPCC Systems. All rights reserved</releaseinfo>

    <date>January 2012 Version 1.0</date>

    <corpname>HPCC Systems</corpname>

    <copyright>
      <year>2012 HPCC Systems. All rights reserved</year>
    </copyright>

    <mediaobject role="logo">
      <imageobject>
        <imagedata fileref="images/ML_COVER_Rightjustified.jpg" />
      </imageobject>
    </mediaobject>
  </bookinfo>

  <chapter id="Introduction">
    <title id="Machine_Learning_Algorithms">Machine Learning
    Algorithms</title>

    <para>The Lexis Nexis machine learning library contains an extensible
    collection of machine learning routines which are easy and efficient to
    use and are designed to execute in parallel across a cluster. The list of
    modules supported will continue to grow over time. The following modules
    are currently supported:</para>

    <itemizedlist>
      <listitem>
        <para>Associations (ML.Associate)</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para>Classify (ML.Classify)</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para>Cluster (ML.Cluster)</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para>Correlations (ML.Correlate)</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para>Discretize (ML.Discretize)</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para>Distribution (ML.Distribution)</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para>Field Aggregates (ML.FieldAggregates)</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para>Regression (ML.Regression)</para>
      </listitem>
    </itemizedlist>

    <para>The Machine Learning modules are supported by the following which
    are also used to implement ML:</para>

    <itemizedlist>
      <listitem>
        <para>The Matrix Library (Mat)</para>
      </listitem>

      <listitem>
        <para>Utility (ML.Utility) </para>
      </listitem>

      <listitem>
        <para>Docs (ML.Doc)</para>
      </listitem>
    </itemizedlist>

    <para>The ML Modules are used in conjunction with the Lexis Nexis HPCC
    system. More information about the HPCC System and it's components is
    available on the following website, <ulink
    url="http://hpccsystems.com/"><ulink
    url="http://hpccsystems.com/">http://hpccsystems.com/</ulink></ulink>.</para>

    <sect1>
      <title id="ML_Data_Models">The ML Data Models</title>

      <para>The ML routines are all centered around a small number of core
      processing models. As a user of ML (rather than an implementer) the
      exact details of these models can generally be ignored. However, it is
      useful to have some idea of what is going on and what routines are
      available to help you with the various models. The formats that are
      shared between various modules within ML are all contained within the
      Type definition.</para>

      <sect2>
        <title id="Numeric_Field">Numeric field</title>

        <para>The principle type that under girds most of the ML processing is
        the Numeric Field which is a general representation of an arbitrary
        ECL record of numeric entries. The record has 3 fields:</para>

        <orderedlist>
          <listitem>
            <para>Id (the ‘record id). An identifier for the record being
            modeled; it will be shared between all of the fields of the
            record.</para>
          </listitem>

          <listitem>
            <para>The field number. An ECL record with 10 fields which
            produces 10 ‘numericfield’ records, one with each of the field
            numbers from 1 to 10.</para>
          </listitem>

          <listitem>
            <para>Value. The value of the field.</para>
          </listitem>
        </orderedlist>

        <para>This is perhaps visualized by making a comparison with a
        traditional ECL record. This simple example shows some height, weight
        and age facts for certain individuals:</para>

        <para><programlisting>IMPORT ml;

value_record := RECORD
                UNSIGNED rid;
  REAL height;
                REAL weight;
                REAL age;
                INTEGER1 species; // 1 = human, 2 = tortoise
                INTEGER1 gender; // 0 = unknown, 1 = male, 2 = female
  END;

d := dataset([{1,5*12+7,156*16,43,1,1},
                                                               {2,5*12+7,128*16,31,1,2},
                                                               {3,5*12+9,135*16,15,1,1},
                                                               {4,5*12+7,145*16,14,1,1},
                                                               {5,5*12-2,80*16,9,1,1},
                                                               {6,4*12+8,72*16,8,1,1},
                                                               {7,8,32,2.5,2,2},
                                                               {8,6.5,28,2,2,2},
                                                               {9,6.5,28,2,2,2},
                                                               {10,6.5,21,2,2,1},
                                                               {11,4,15,1,2,0},
                                                               {12,3,10.5,1,2,0},
                                                               {13,2.5,3,0.8,2,0},
                                                               {14,1,1,0.4,2,0}
                                                               ]
                                                               ,value_record);

d;
</programlisting>There are 14 rows of data. Each row has 5 interesting data
        fields and a ‘record id’ that is prepended to uniquely identify the
        record. ML provides the ToField operation that converts a record in
        this general format to the NumericField format:</para>

        <para><programlisting>ml.ToField(d,o);
d;
o
</programlisting>This shows the original data but also the data in the
        standard ML NumericField format. The latter has 70 rows (5x14). If a
        file has N rows and M columns then the order of the ToField operation
        will be O(mn).</para>

        <para>It is also possible to turn the NumericField format back into a
        ‘regular’ ECL style record using the FromField operation:</para>

        <para><programlisting>ml.ToField(d,o);
d;
o;
ml.FromField(o,value_record,d1);
d1;

Will leave d1 = d
</programlisting></para>

        <para>By default, the ToField operation assumes the first field is the
        “id” field, and all subsequent numeric fields are to be assigned a
        field number in the resulting table. However, additional parameters
        may be specified in the ToField that facilitates the ability to
        specify the name of the id column in the original table as well as the
        columns to be used as data fields.</para>

        <para>For example:</para>

        <para><programlisting>IMPORT ML;
value_record := RECORD
  STRING first_name;
  STRING last_name;
  UNSIGNED name_id;
  REAL height;
  REAL weight;
  REAL age;
  STRING eye_color;
  INTEGER1 species; // 1 = human, 2 = tortoise
  INTEGER1 gender; // 0 = unknown, 1 = male, 2 = female
END;

d := dataset([
  {'Charles','Babbage',1,5*12+7,156*16,43,'Blue',1,1},
  {'Tim','Berners-Lee',2,5*12+7,128*16,31, 'Brown',1,1},
  {'George','Boole',3,5*12+9,135*16,15, 'Hazel',1,1},
  {'Herman','Hollerith',4,5*12+7,145*16,14,'Green',1,1},
  {'John','Von Neumann',5,5*12-2,80*16,9,'Blue',1,1},
  {'Dennis','Ritchie',6,4*12+8,72*16,8, 'Brown',1,1},
  {'Alan','Turing',7,8,32,2.5, 'Brown',2,1}
],value_record);

ML.ToField(d,o,name_id,'height,weight,age,gender',m);
d;
o;
m;

</programlisting>In the above example, the name_id column is taken as the id.
        Height, weight, age and gender will be parsed into numbered fields.
        The last parameter that was passed was the name of a mapping table
        that may be optionally requested. For the above example, the mapping
        table looks like this:</para>

        <informaltable>
          <tgroup cols="2">
            <tbody>
              <row>
                <entry>first_name</entry>

                <entry>NA</entry>
              </row>

              <row>
                <entry>last_name</entry>

                <entry>NA</entry>
              </row>

              <row>
                <entry>name_id</entry>

                <entry>ID</entry>
              </row>

              <row>
                <entry>height</entry>

                <entry>1</entry>
              </row>

              <row>
                <entry>weight</entry>

                <entry>2</entry>
              </row>

              <row>
                <entry>age</entry>

                <entry>3</entry>
              </row>

              <row>
                <entry>eye_color</entry>

                <entry>NA</entry>
              </row>

              <row>
                <entry>species</entry>

                <entry>NA</entry>
              </row>

              <row>
                <entry>gender</entry>

                <entry>4</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable>

        <para>The mapping table may be used when reconstituting the data back
        to the original format, for example:</para>

        <para><programlisting>ML.FromField(o,value_record,r,m);
r;
</programlisting>The output from this FromField call will have the same
        structure as the initial table and values that existed in the
        NumericField version of the table will be allocated to the fields
        specified in the mapping table. An important note is that any data
        that did not translate into the NumericField table will be left blank
        or zero in the reconstituted table.</para>

        <para>(Editor’s note: The authors do not actually know the true eye
        color of the people named above. However, it did come as a surprise to
        discover that Alan Turing was in fact a tortoise!)</para>
      </sect2>

      <sect2>
        <title id="Discrete_field">Discrete field</title>

        <para>Some of the ML routines do not require the field values to be
        real, rather they require discrete (integral) values. The structure of
        the records are essentially identical to NumericField but, the value
        is of type t_Discrete (typically INTEGER) rather than t_FieldReal
        (typically REAL8). There are no explicit routines to get to a
        discrete-field structure from an ECL record, rather it is presumed
        that NumericField will be used as an intermediary. There is an entire
        module (Discretize) devoted to moving a NumericField structured file
        into a DiscreteField structured file. The options and reasons for the
        options are described in the Discretize module section. For this
        introduction it is adequate to show that all of the numeric fields
        could be made integral simply by using:</para>

        <para><programlisting>ml.ToField(d,o);
o;
o1 := ML.Discretize.ByRounding(o);
o1</programlisting></para>
      </sect2>

      <sect2>
        <title id="ItemElement">ItemElement</title>

        <para>A rather more specialist format is the ItemElement format. This
        does not model an ECL record directly, rather it models an abstraction
        that can be derived from an ECL record. The item element has a record
        id and a value (which is of type t_Item). The t_Item is an integral
        value – but unlike t_Discrete the values are not considered to be
        ordinal. Put another way, in t_Discrete 4 &gt; 3 and 2 &lt; 3. In
        t_Item the 2, 3, 4 are just arbitrary labels that ‘happen’ to be
        integers for efficiency. Note also, that ItemElement does not have a
        field number. There is no significance placed upon the field from
        which the value was derived. This models the abstract notion of a
        collection of ‘bags’ of items. An example of the use of this type of
        structure will be given in the Using ML with documents,
        section.</para>
      </sect2>

      <sect2>
        <title id="Coding_ML_Data_Models">Coding with the ML data
        models</title>

        <para>The ML data models are extremely flexible to work with; but
        using them is a little different from traditional ECL programming.
        This section aims to detail some of the possibilities.</para>

        <sect3>
          <title id="Column_Splitting">Column splitting</title>

          <para>Some of the ML routines expect to be handed two datasets which
          may be, for example, a dataset of independent variables and another
          of dependent variables. The data as it originally exists will
          usually have the independent and dependant data within the same row.
          For example, when using a classifier to produce a model to predict
          the species or gender of an entity from the other details, the
          height, weight and age fields would need to be in a different ‘file’
          to the species and gender. However, they have to have the same
          record ID to show the correlation between the two. In the ML data
          model this is as simple as applying two filters: <programlisting>ml.ToField(d,o);
o1 := ML.Discretize.ByBucketing(o,5);
Independents := o1(Number &lt;= 3);
Dependents := o1(Number &gt;= 4);
Bayes := ML.Classify.BuildNaiveBayes(Independents,Dependents);
Bayes
</programlisting></para>
        </sect3>

        <sect3>
          <title id="Genuine_Nulls">Genuine nulls</title>

          <para>Implementing a genuine null can be done by simply removing
          certain fields with certain values from the datastream. For example,
          if 0 was considered an invalid weight then one could
          do:<programlisting>Better := o(Number&lt;&gt;2 OR Value&lt;&gt;0);</programlisting></para>
        </sect3>

        <sect3>
          <title id="Sampling">Sampling</title>

          <para>By far the easiest way to split a single data file into
          samples is to use the SAMPLE and ENTH verbs upon the datafile PRIOR
          to the conversion to ML format.</para>
        </sect3>

        <sect3>
          <title id="Inserting_Column_Computed_Value">Inserting a column with
          a computed value</title>

          <para>Inserting a column with a new value computed from another
          field value is a fairly advanced technique. The following inserts
          the square of the weight as a new column:<programlisting>ml.ToField(d,o);

BelowW := o(Number &lt;= 2); 
// Those columns whose numbers are not changed
// Shuffle the other columns up - this is not needed if appending a column
AboveW := PROJECT(o(Number&gt;2),TRANSFORM(ML.Types.NumericField,SELF.Number := 
LEFT.Number+1, SELF := LEFT));
NewCol := PROJECT(o(Number=2),TRANSFORM(ML.Types.NumericField,
                                                        SELF.Number := 3,
                                                        SELF.Value := LEFT.Value*LEFT.Value,
                                                        SELF := LEFT) );
    
NewO := BelowW+AboveW+NewCol;

NewO;
</programlisting></para>
        </sect3>
      </sect2>
    </sect1>

    <sect1 id="Generating_test_data">
      <title id="Generating_test_data">Generating test data</title>

      <para>ML is interesting when it is being executed against data with
      meaning and significance. However, sometimes it can be useful to get
      hold of a lot of data quickly for testing purposes. This data may be
      ‘random’ (by some definition) or it may follow a number of carefully
      planned statistical distributions. The ML libraries have support for
      high performance ‘random value’ generation using the GenData command
      inside the distribution module.</para>

      <para>GenData generates one column at a time although it generates that
      column for all the records in the file. It works in parallel so is very
      efficient.</para>

      <para>The easiest type of column to generate is one in which the values
      are evenly and randomly distributed over a range. The following
      generates 1M records each with a random number from 0-100 in the first
      column:</para>

      <para><programlisting>IMPORT ML;

TestSize := 1000000;
a1 := ML.Distribution.Uniform(0,100,10000); 
ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform

</programlisting></para>

      <para>To generate 1M records with three columns; one Uniformly
      distributed, one Normally distributed (mean 0, Standard Deviation 10)
      and one with a Poisson distribution (Mean of 4):</para>

      <programlisting>IMPORT ML;

TestSize := 1000000;

a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
// Field 2 Normally Distributed
a2 := ML.Distribution.Normal2(0,10,10000);
b2 := ML.Distribution.GenData(TestSize,a2,2);
// Field 3 - Poisson Distribution
a3 := ML.Distribution.Poisson(4,100);
b3 := ML.Distribution.GenData(TestSize,a3,3);

D := b1+b2+b3; // This is the test data

ML.FieldAggregates(D).Simple;  // Perform some statistics on the test data to ensure 
                                  it worked
</programlisting>

      <para>This generates the data in the correct format and even produces
      some statistics to ensure it works!</para>

      <para>The ML libraries have over half a dozen different distributions
      that the generated data columns can be given. These are described at
      length in the Distribution module section.</para>
    </sect1>
  </chapter>

  <chapter id="ML_module_walkthroughs">
    <title id="ML_module_walkthroughs">ML module walk-throughs</title>

    <para>To help you get started, a walk-through is provided for each ML
    module. The walk-throughs explain how the modules work and demonstrate how
    they can be used to generate the results you require.</para>

    <sect1 id="Classify_walkthrough">
      <title id="Classify_walkthrough">Classification walk-through</title>

      <para><emphasis role="bold">Modules: Classify, Regression,
      Doc</emphasis></para>

      <para>Classification techniques tackle the problem of identifying which
      groups new observations belong to, based on learned characteristics of
      an equivalent training set.</para>

      <para>Based upon a set of observations and groups provided in a training
      set, can we correctly predict the group of an observation outside of the
      training set.</para>

      <para>This is really where data processing gives way to machine
      learning. Based upon some form of training set can I derive a rule or
      model to predict something about other data records?</para>

      <para>Classification is sufficiently central to machine learning that we
      provide three different methods of doing it. You will need to examine
      the literature or experiment to decide exactly which method of
      classification will work best in any given context. In the examples that
      follow we are simply trying to show how a given method can be used in a
      given context and we are not necessarily claiming it is the best or only
      way to solve the given example.</para>

      <sect2 id="Naive_Bayes">
        <title id="Naive_Bayes">Naive Bayes</title>

        <para>The concept behind Naïve Bayes is that every property of an
        object is in and of itself a predictor of the type of that object. By
        summing all of the predictions for all of the properties of an object,
        you can compute the type the object is most likely to have. Put
        another way, Bayes assumes that all of the properties of an object are
        independent hence the expression ‘Naïve’.</para>

        <para>Another interesting feature (and benefit) of Bayes is that while
        it predicts ordinal values (integers – 0, 1, 2 etc), it is not
        producing a ‘score’ so, it does not restrict the integers to being ‘in
        sequence’. So a property of an object might predict a 0 or 2, but it
        will not predict a 1.</para>

        <para>Bayes will not predict anything if handed totally random data.
        It is precisely looking for a relationship between the data that is
        non-random. The following example generates test data by:</para>

        <orderedlist>
          <listitem>
            <para>Generating three random columns.</para>
          </listitem>

          <listitem>
            <para>Producing a fourth column that is the sum of the three
            columns.</para>
          </listitem>

          <listitem>
            <para>Giving the fourth column a category from 0 (small) to 2
            (big).</para>
          </listitem>
        </orderedlist>

        <para>The purpose is to see if the system can learn to predict which
        category the record will be assigned using the individual fields. The
        data generation is a little more complex than normal so it is
        presented here. In the rest of this section if a ‘D1’ appears from
        nowhere, it is referencing this dataset.</para>

        <para><programlisting>IMPORT ML;

TestSize := 10000000;

a1 := ML.Distribution.Poisson(5,100); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
a3 := ML.Distribution.Poisson(3,100);
b3 := ML.Distribution.GenData(TestSize,a3,3);

D := b1+b2+b3; // This is the test data
// Now construct a fourth column which is the sum of them all
B4 := PROJECT(TABLE(D,{Id,Val := SUM(GROUP,Value)},Id),TRANSFORM(ML.Types.NumericField,
                                           SELF.Number:=4,
                                           SELF.Value:=MAP(LEFT.Val &lt; 6 =&gt; 0,  // Small
                                                                    &lt; 10 =&gt; 1, // Normal
                                                                          2 ); // Big
                                           SELF := LEFT));

D1 := D+B4;
</programlisting>When generating and preparing data for Naïve Bayes you need
        to be aware that Bayes is looking for discrete numbers as input (not
        the real numbers generated). The Discretize module can do this for
        you. This module is described in detail in the chapter The ML
        Modules.</para>

        <para>To keep things simple we will just round all the fields to
        integers in this example:</para>

        <para><programlisting>D2 := ML.Discretize.ByRounding(D1);</programlisting>It
        is now possible to ‘train’ the Naïve Bayes classifier using the
        BuildNaiveBayes definition:<programlisting>ML.Classify.BuildNaiveBayes(D2(Number&lt;=3),D2(Number=4))</programlisting></para>

        <para>Notice that all of the values I ‘know’ are being passed in as
        the first parameter (these are the independent variables). All of the
        ones I want to learn (the dependents) are being passed in as the
        second parameter. In this case there is only one column of dependent
        variables but the BuildNaiveBayes is capable of constructing multiple
        Bayesian models at once.</para>

        <para>One thing that seems to shock some people is that a NaiveBayes
        classifier is not perfect. Even if you construct a Bayes model, you
        can hand the classifier back the training data and it may still
        mis-classify some of the data. This is not a bug. It is a restriction
        of the mathematical model that underlies Bayes. This obviously begs
        the question, "well, it may not be perfect but just how good is
        it?".</para>

        <para>The Classify module has a definition, TestNaiveBayes, which
        takes in a set of independent variables, a set of actual outcomes
        (dependent variables) and a Bayes model. It will then return a slew of
        results to show how good (or bad) the classifier was. The following
        results are produced:</para>

        <table>
          <title></title>

          <tgroup cols="2">
            <colspec align="center" />

            <thead>
              <row>
                <entry align="center">Result</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Headline</entry>

                <entry>Gives you the main precision number. On this test data
                how often was the classifier correct.</entry>
              </row>

              <row>
                <entry>PrecisionByClass</entry>

                <entry>Similar to Headline except that it gives the precision
                broken down by the class that it SHOULD have been classified
                to. It is possible that a classifier might work well in
                general but may be particularly poor at identifying one of the
                groups.</entry>
              </row>

              <row>
                <entry>CrossAssignments</entry>

                <entry>It is one thing to say a classification is ‘wrong’.
                This table is there to show, “if a particular class is
                mis-classified, what is it most likely to be mis-classified
                as?”.</entry>
              </row>

              <row>
                <entry>Raw</entry>

                <entry>Gives a very detailed breakdown of every record in the
                test corpus such as what the classification should have been
                and what it was.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>Assuming you are happy with your model the final step is to use
        it! This is very easy to do because the Classify module has a
        NaiveBayes definition which takes the independent data and the model
        you supply and produces the dependent columns you require. In the
        following example, this is done using the training set we used ealier.
        For real work this would be a completely different dataset that was
        being processed:</para>

        <para><programlisting>Model := ML.Classify.BuildNaiveBayes(D2(Number&lt;=3),D2(Number=4));
Results := ML.Classify.NaiveBayes(D2(Number&lt;=3),Model);
Results 
</programlisting></para>

        <para></para>
      </sect2>

      <sect2 id="Logisitic_regression">
        <title id="Logisitic_regression">Logistic Regression </title>

        <para>Regression analysis includes techniques for modeling the
        relationship between a dependent variable Y and one or more
        independent variables Xi.</para>

        <para>The ML.Regression module currently provides two models, OLS and
        Poly, where the relationship is expressed as linear and polynomial
        function respectively.</para>

        <para>When dependent variable Y is binary, ie when Y takes either 0 or
        1 values, neither linear nor polynomial function can model the
        relationship between X and Y correctly. In that case, the relationship
        can be modeled using a logistic function, also known as sigmoid
        function, which is an S-shaped curve with values from (0,1).</para>

        <para>Since dependent variable Y can take only two values, 0 or 1, the
        Logistic Regression model predicts two outcomes 0 or 1, and it can be
        used as a tool for classification.</para>

        <para><programlisting>IMPORT ML;

value_record:= RECORD
    unsigned   rid;
    real       age;
    real       height;
    integer1   sex; // 0 = female, 1 = male
END;

d := DATASET([{1,35,149,0},{2,11,138,0},{3,12,148,1},{4,16,156,0},
            {5,32,152,0},{6,16,157,0},{7,14,165,0},{8,8,152,1},
            {9,35,177,0},{10,33,158,1},{11,40,166,0},{12,28,165,0},
            {13,23,160,0},{14,52,178,1},{15,46,169,0},{16,29,173,1},
            {17,30,172,0},{18,21,163,0},{19,21,164,0},{20,20,189,1},
            {21,34,182,1},{22,43,184,1},{23,35,174,1},{24,39,177,1},
            {25,43,183,1},{26,37,175,1},{27,32,173,1},{28,24,173,1},
            {29,20,162,0},{30,25,180,1},{31,22,173,1},{32,25,171,1}]
            ,value_record);

ML.ToField(d,flds);
logistic := ML.Classify.Logistic(flds(Number=2),flds(Number=3));
logistic.Beta;
logistic.modelY;
</programlisting>In this example we have a dataset containing age, height in
        centimeters and sex of 32 people, and we would like to create a
        logistic model that will make it possible to predict a person's sex
        given a person's height. The logistic model is built using the height
        column as the independent variable, and using the sex column as the
        dependent variable. The logistic.Beta calculates parameters of the
        model using the iteratively-reweighted least squares (IRLS) algorithm:
        β= (β0, β1), and those parameters can be plugged into the model
        function Y = 1 ./ (1+exp(-X*β)) to predict the sex of a person X with
        the specified height. If the model-calculated value Y is greater than
        0.5, then the sex of the person X is predicted to be male, otherwise
        the sex is predicted to be female. The logistic.modelY returns the
        model-calculated (ie predicted) sex for all the heights provided by
        the independent variable X.</para>
      </sect2>
    </sect1>

    <sect1>
      <title id="Cluster_Walkthrough">Cluster Walk-through</title>

      <para><emphasis role="bold">Modules: Cluster, Doc</emphasis></para>

      <para>The cluster module contains routines that can be used to find
      groups of records that appear to be ‘fairly similar’. The module has
      been shown to work on records with as few as two fields and as many as
      sixty thousand; the latter was used for clustering documents of words
      (see Using ML with documents). The clustering module has more than half
      a dozen different ways of measuring the distance (defining ‘similar’)
      between two records but it is also possible to write your own.</para>

      <para>Below are walk-throughs for the methods covered in the ML.Cluster
      module. Each begins with the following set of entities in 2-dimensional
      space, where the values on each axis are restricted to between 0.0 and
      10.0:</para>

      <para><programlisting>IMPORT ML;

lMatrix:={UNSIGNED id;REAL x;REAL y;};

dEntityMatrix:=DATASET([
  {1,2.4639,7.8579},
  {2,0.5573,9.4681},
  {3,4.6054,8.4723},
  {4,1.24,7.3835},
  {5,7.8253,4.8205},
  {6,3.0965,3.4085},
  {7,8.8631,1.4446},
  {8,5.8085,9.1887},
  {9,1.3813,0.515},
  {10,2.7123,9.2429},
  {11,6.786,4.9368},
  {12,9.0227,5.8075},
  {13,8.55,0.074},
  {14,1.7074,3.9685},
  {15,5.7943,3.4692},
  {16,8.3931,8.5849},
  {17,4.7333,5.3947},
  {18,1.069,3.2497},
  {19,9.3669,7.7855},
  {20,2.3341,8.5196}
],lMatrix);

ML.ToField(dEntityMatrix,dEntities);
</programlisting><note>
          <para>The use of the ToField macro which converts the original
          rectangular matrix, dEntityMatrix, into a table in the standard
          NumericField format used by the ML library named “dEntities”.</para>
        </note></para>

      <sect2>
        <title id="kmeans">KMeans</title>

        <para>With k-means clustering the user creates a second set of
        entities called centroids, with coordinates in the same space as the
        entities being clustered. The user defines the number of centroids (k)
        to create, which will remain constant during the process and therefore
        represents the number of clusters that will be determined. For our
        example, we will define four centroids:</para>

        <para><programlisting>dCentroidMatrix:=DATASET([
  {1,1,1},
  {2,2,2},
  {3,3,3},
  {4,4,4}
],lMatrix);

ML.ToField(dCentroidMatrix,dCentroids);
</programlisting>As with the entity matrix, we have used ToField to convert
        the centroid matrix into the table “dCentroids”.</para>

        <para>Note that although these points are arbitrary, they are clearly
        not random. These points form an asymmetrical pattern in one corner of
        the space. This is to highlight a feature of k-means clustering which
        is that the centroids will end up in the same resting place (or very
        close to it) regardless of where they started. The only caveat related
        to centroid positioning is that no two centroids should occupy the
        same initial location.</para>

        <para>Now that we have our centroids, they are now subjected to a
        2-step iterative re-location process. For each iteration we determine
        which entities are closest to which centroids, then we recalculate the
        position of the centroids based as the mean location of all of the
        entities affiliated with them.</para>

        <para>To set up this process, we make the following call to the KMeans
        routine:</para>

        <para><programlisting>MyKMeans:=ML.Cluster.KMeans(dEntities,dCentroids,30,.3);
</programlisting>Here, we are passing in our two datasets, dEntities and
        dCentroids. In order to prevent infinite loops, we also must specify a
        maximum number of iterations, which is set to 30 in the above
        example.</para>

        <para>Convergence is defined as the point at which we can say the
        centroids have found their final resting places. Ideally, this will be
        when they stop moving completely. However, there will be situations
        where centroids may experience a “see-sawing” action, constantly
        trading affiliations back and forth indefinitely. To address this, we
        have the option of specifying a positive value as the convergence
        threshold. The process will assume convergence if, during any
        iteration, no centroid moves a distance greater than that number. In
        our above example, we are setting the convergence threshold to 0.3. If
        no threshold is specified, then the threshold is set to 0.0. If the
        process hits the maximum number of iterations passed in as parameter
        3, then it stops regardless of whether convergence is achieved or
        not.</para>

        <para>The final parameter, which is also optional, specifies which
        distance formula to use. For our example we are leaving this parameter
        blank, so it defaults to a simple Euclidean calculation, but we could
        easily change this by adding the fifth parameter with a value such as
        “ML.Cluster.DF.Tanimoto” or “ML.Cluster.DF.Manhattan”.</para>

        <para>Below are calls to the available attributes within the KMeans
        module:</para>

        <para><programlisting>MyKMeans.AllResults;
</programlisting>This will produce a table with a layout similar to
        NumericField, but instead of a single value field, we have a field
        named “values” which is a set of values. Each row will have the same
        number of values in this set, which is equal to the number of
        iterations + 1. Values[1] is the initial value for the id/number
        combination, Values[2] is after the first iteration, etc.</para>

        <para><programlisting>MyKMeans.Convergence;</programlisting>Convergence
        will respond with the number of iterations that were performed, which
        will be an integer between 1 and the maximum specified in the
        parameters. If it is equal to the maximum, then you may want to
        increase that number or specify a higher convergence threshold because
        it had not yet achieved convergence when it completed.</para>

        <para><programlisting>MyKMeans.Result();  // The final locations of the centroids
MyKMeans.Result(3); // The results of iteration 3
</programlisting>Result will respond with the centroid locations after the
        specified number of iterations. If no number is passed, this will be
        the locations after the final iteration.</para>

        <para><programlisting>MyKMeans.Delta(3,5); // The distance every centroid travelled across each axis from 
                        iterations 3 to 5
MyKMeans.Delta(0);   // The total distance the centroids travelled on each axis from the 
                        beginning to the end
</programlisting>Delta displays the distance traveled <emphasis>on each
        axis</emphasis> between the iterations specified in the parameters. If
        no parameters are passed, this will be the delta between the last two
        iterations.</para>

        <para><programlisting>MyKMeans.DistanceDelta(3,5); // The straight-line distance travelled by each centroid from 
                                iterations 3 to 5
MyKMeans.DistanceDelta(0);   // The total straight-line distance each centroid travelled 
MyKMeans.DistanceDelta();    // The distance traveled by each centroid during the last 
                                iteration.
</programlisting>DistanceDelta is the same as Delta, but displays the DISTANCE
        delta as calculated using whichever method the KMeans routine was
        instructed to use, which in our example is Euclidean.</para>
      </sect2>

      <sect2>
        <title id="AggloN">AggloN</title>

        <para>With Agglomerative, or Hierarchical, clustering there is no need
        for a centroid set. This method takes a bottom-up approach whereby it
        identifies those pairs that are mutually closest and marries them so
        they are treated as a single entity during the next iteration. Allowed
        to run until full convergence, every entity will eventually be
        stitched up into a single tree structure with each fork representing
        tighter and tighter clusters.</para>

        <para>We set up this clustering routine using the following
        call:</para>

        <para><programlisting>MyAggloN:=ML.Cluster.AggloN(dEntities,4);</programlisting>Here,
        we are passing in our sample data set and telling the routine that we
        want a maximum of 4 iterations.</para>

        <para>There are two further parameters that the user may pass, both of
        which are optional. Parameter 3 enables the user to specify the
        distance formula exactly as we could in Parameter 5 of the KMeans
        routine. And as with our KMeans example, we will leave this blank so
        it defaults to Euclidean.</para>

        <para>Parameter 4 enables us to specify how we want to represent
        distances where clustered entities are involved. After the first
        iteration some of the entities will have been grouped together and we
        need to make a decision about how we measure distance to those groups.
        The three options are min_dist, max_dist, and ave_dist, which will
        instruct the routine to use the minimum distance within the cluster,
        the maximum or the average respectively. The default, which we are
        accepting for this example, is min_dist.</para>

        <para>The following three calls will give us the results of the
        Agglomerative clustering call in different ways:</para>

        <para><programlisting>MyAggloN.Dendrogram;</programlisting>The
        Dendrogram call displays the output as a string representation of the
        tree diagram. Clusters are grouped within curly braces ({}), and
        clusters of clusters are grouped in the same manner. The ID for each
        cluster will be assigned the lowest ID of the entities it encompasses.
        In our example, we end up with five clusters, and two entities yet to
        be clustered. This is because we specified a maximum of four
        iterations which was not enough to group everything together.</para>

        <para><programlisting>MyAggloN.Distances;</programlisting>The
        Distances output displays all of the remaining distances that would be
        used to further cluster the entities. If we had achieved convergence,
        this would be an empty table and our Dendrogram output would be a
        single line with every item found within the tree string. But since we
        stopped iterating early, we still have items to cluster, and therefore
        still have distances to display. The number of rows here will be equal
        to n*n-1, where n is the number of rows in the Dendrogram
        table.</para>

        <para><programlisting>MyAggloN.Clusters;</programlisting>Clusters will
        display each entity, and the ID of the cluster that the entity was
        assigned to. In our example, every entity will be assigned to one of
        the seven cluster IDs found in the Dendrogram. If we had allowed the
        process to continue to convergence, which for our sample set is
        achieved after 9 iterations, every entity will be assigned the same
        cluster ID because it will be the only one left in the
        Dendrogram.</para>
      </sect2>
    </sect1>

    <sect1>
      <title id="Colocation_Walkthrough">CoLocation Walkthrough</title>

      <para><emphasis role="bold">Modules: Docs</emphasis></para>

      <para>The Docs/CoLocation module exists to perform textual analysis on a
      corpus of documents. It harvests n-grams and enables the user to perform
      analysis on those n-grams to determine significance.</para>

      <para>For the purposes of this walk-through we will be using the
      following limited dataset:</para>

      <para><programlisting>IMPORT ML.Docs AS Docs;

d:=DATASET([
  {1,'David went to the market and bought milk and bread'},
  {2,'John picked up butter on his way home from work.'},
  {3,'Jill craved lemon cookies, so she grabbed some at the convenience store'},
  {4,'Mary needs milk, bread and butter to make breakfast tomorrow morning.'},
  {5,'William's lunch included a sandwich on wheat bread and chocolate chip cookies.'}
],Docs.Types.Raw);
</programlisting>The format of the initial dataset is in the Raw format in
      Docs.Types, which is a simple numeric ID and a string of free text of
      indeterminate length.</para>

      <para>The first step in processing the free text is to map all of the
      words. We do this by calling the Words attribute, which will utilize
      elements of the Tokenize module to break all of the text out into its
      elements:</para>

      <para><programlisting>Words:=Docs.CoLocation.Words(d);</programlisting>The
      AllNGrams attribute will then harvest every n-gram, from unigrams up to
      the n defined by the user. This will result in a table that contains a
      row for every unique id/n-gram combination. In the following line, we
      are asking for anything up to a 4-gram. If the n parameter is left
      blank, the default is 3.</para>

      <para>Note that the second parameter has been left blank and will be
      covered a little later.</para>

      <para><programlisting>AllNGrams:=Docs.CoLocation.AllNGrams(Words,,4);</programlisting>Below
      are calls to the standard metrics that are currently built into the
      CoLocation module. Note that the call to Words above has converted all
      characters in the text to uppercase:</para>

      <para><programlisting>// SUPPORT: User passes a SET OF STRING and the output from the ALLNGrams attribute
Docs.CoLocation.Support(['MILK','BREAD','BUTTER'],AllNGrams);

// CONFIDENCE, LIFT and CONVICTION: User passes in two SETS OF STRING and the AllNGrams 
   output.
// In each case, set 1 and set 2 are read as “1=&gt;2”.  Note that 1=&gt;2 DOES NOT EQUAL 2=&gt;1.
Docs.CoLocation.Confidence(['MILK','BREAD'],['BUTTER'],AllNGrams);
Docs.CoLocation.Lift(['MILK','BREAD'],['BUTTER'],AllNGrams);
Docs.CoLocation.Conviction(['MILK','BREAD'],['BUTTER'],AllNGrams);

</programlisting>To further distill the data the user may call NGrams, which
      strips the document IDs and groups the table so that there is one row
      per unique n-gram. Included in this output is aggregate information
      including the number of documents in which the item appears, the
      percentage of that compared to the document count, and the Inverse
      Document Frequency (IDF).</para>

      <para><programlisting>NGrams:=Docs.CoLocation.NGrams(AllNGrams);</programlisting>With
      the output from NGrams we have other attributes that we can call to
      further analyze the data.</para>

      <para>Calling SubGrams will produce a table of every n-gram where n&gt;1
      along with a comparison of the document frequency of the n-gram to the
      product of the frequencies of all of its constituent unigrams. This will
      give an indication of whether the phrase or its parts may be more
      significant in the context of the corpus.</para>

      <para><programlisting>Docs.CoLocation.SubGrams(NGrams);</programlisting>Another
      measure of significance is SplitCompare. This splits every n-gram with
      n&gt;1 into two rows with two parts: The initial unigram and the
      remainder, and the final unigram and the remainder. The document
      frequencies of all three items (the full n-gram, and the two constituent
      parts) are then presented side-by-side so their relative values can be
      evaluated. This would help to determine if a leading or trailing word
      carries any weight in the encompassing phrase.</para>

      <para><programlisting>Docs.CoLocation.SplitCompare(NGrams);</programlisting></para>

      <sect2>
        <title id="Integer_Replacement">Integer replacement</title>

        <para>In order to deal with very large data sets, the facility exists
        to perform integer replacement of the words so that they take up less
        space. This is done with a call to the Lexicon attribute, which uses
        the Tokenize.Lexicon function to aggregate the n-grams and assign a
        unique integer value to each unique item. The process will assign the
        most frequent item the lowest number, so “The” or “A” will usually
        take the first position in English.</para>

        <para><programlisting>Lexicon:=Docs.CoLocation.Lexicon(Words);</programlisting>Here
        is where the missing parameter from our AllNGrams call comes in. If we
        specify a Lexicon as the second parameter, the output from AllNGrams
        produces the n-grams after they have been substituted for their
        integer equivalents. So phrases such as “Chocolate Chip Cookies” will
        be converted to something like “14 13 4”, where “Chocolate” is
        assigned the number 14, etc. Clearly, this saves a lot of space and
        will also save processing time when subsequent attributes perform
        string manipulation actions against it. Since the attributes are all
        statistical in nature, the substitution has no bearing on the
        outcomes.</para>

        <para><programlisting>AllNGramsWithLexicon:=Docs.CoLocation.AllNGrams(Words,Lexicon,4);
NGramsWithLexicon:=Docs.CoLocation.NGrams(AllNGramsWithLexicon);
AllNGramsWithLexicon;
</programlisting>Once any analysis has been done and the user has phrases of
        significance, they can be re-constituted using a call to
        ShowPhrase:</para>

        <para><programlisting>Docs.CoLocation.ShowPhrase(Lexicon,’14 13 4’); // would return ‘CHOCOLATE CHIP COOKIES’</programlisting></para>
      </sect2>
    </sect1>

    <sect1 id="Field_aggregates_walkthrough">
      <title id="Field_aggregates_walkthrough">Field Aggregates
      Walkthrough</title>

      <para><emphasis role="bold">Modules: FieldAggregates,
      Distribution</emphasis></para>

      <para>The FieldAggregates module exists to provide statistics upon each
      of the fields of a file. The file is passed in to the field aggregates
      module and then various properties of those fields can be queried, for
      example:</para>

      <para><programlisting>IMPORT ML;
// Generate random data for testing purposes
TestSize := 10000000;
a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
D := b1+b2; // This is the test data
// Pass the test data into the Aggregate Module
Agg := ML.FieldAggregates(D);
Agg.Simple; // Compute some common statistics
</programlisting>This example provides two rows. The ‘number’ column ties the
      result back to the column being passed in. There are columns for
      min-value, max-value, the sum, the number of rows (with values), the
      mean, the variance and the standard deviation.</para>

      <para>The ‘simple’ attribute is a very good one to use on huge data as
      it is a simple linear process.</para>

      <para>The aggregate module is also able to ‘rank order’ a set of data;
      the SimpleRanked attribute allocates every value in every field a number
      – the smallest value gets the number 1, then 2 etc. The ‘Simple’
      indicator is to denote that if a value is repeated the attribute will
      just arbitrarily pick which one gets the lower ranking.</para>

      <para>As you might expect there is also a ‘ranked’ attribute. In the
      case of multiple identical values this will assign every value with the
      same value a rank which is the average value of the ranks of the
      individual items, for example:</para>

      <para><programlisting>IMPORT ML;
TestSize := 50;
a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
D := b1+b2; // This is the test data
Agg := ML.FieldAggregates(D);
Agg.SimpleRanked;
Agg.Ranked;
</programlisting></para>

      <para>Note that ranking requires the data to be sorted; therefore
      ranking is an ‘NlgN’ process.</para>

      <para>When examining the results of the ‘Simple’ attribute you may be
      surprised that two of the common averages ‘median’ and ‘mode’ are
      missing. While the Aggregate module can return those values, they are
      not included in the ‘Simple’ attribute because they are NLgN processes
      and we want to keep ‘Simple’ as cheap as possible.</para>

      <para>The median values for each column can be obtained using the
      following:</para>

      <para><programlisting>Agg.Medians;</programlisting>The modes are found
      by using:<programlisting>Agg.Modes;</programlisting></para>

      <para>It is possible that more than one mode will be returned for a
      particular column, if more than one value has an equal count.</para>

      <para>The final group of features provided by the Aggregate module are
      the NTiles and the Buckets. These are closely related but totally
      different which can be confusing.</para>

      <para>The NTiles are closely related to terms like ‘percentiles’,
      ‘deciles’ and ‘quartiles’, which allow you to grade each score according
      the a ‘percentile’ of the population. The name ‘N’ tile is there because
      you get to pick the number of groups the population is split into. Use
      NTile(4) for quartiles, NTile(10) for deciles and NTile(100) for
      percentiles. NTile(1000) can be used if you want to be able to split
      populations to one tenth of a percent. Every group (or Tile) will have
      the same number of records within it (unless your data has a lot of
      duplicate values because identical values land in the same tile).</para>

      <para>The following example demonstrates the possible use of
      NTiling:</para>

      <para>Imagine you have a file with people and for each person you have
      two columns (height and weight). NTile that file with a number, such as
      100. Then if the NTile of the Weight is much higher than the NTile of
      the Height, the person might be overweight. Conversely if the NTile of
      the Height is much higher than the Weight then the person might be
      underweight. If the two percentiles are the same then the person is
      ‘normal’.</para>

      <para>NTileRanges returns information about the highest and lowest value
      in every Tile. Suppose you want to answer the question: “what are the
      normal SAT scores for someone going to this college”. You can compute
      the NTileRanges(4). Then you can note both the low value of the second
      quartiles and the high value of the third quartile and declare that “the
      middle 50% of the students attending that college score between X and
      Y”. The following example demonstrates this:</para>

      <para><programlisting>IMPORT ML;
TestSize := 100;
a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
D := b1+b2; // This is the test data
Agg := ML.FieldAggregates(D);
Agg.NTiles(4);
Agg.NTileRanges(4)
</programlisting>Buckets provide very similar looking results. However buckets
      do NOT attempt to divide the groups so that the population of each group
      is even. Buckets are divided so that the RANGE of each group is even.
      Suppose that you have a field with a MIN of 0 and MAX of 50 and you ask
      for 10 buckets, the first bucket will be 0 to (almost)5, the second 5 to
      (almost) 10 etc. The Buckets attribute assigns each field value to the
      bucket.</para>

      <para>The BucketRanges returns a table showing the range of each bucket
      and also the number of elements in that bucket. If you wanted to plot a
      histogram of value verses frequency, for example, buckets would be the
      tool to use.</para>

      <para>The final point to mention is that many of the more sophisticated
      measures use the simpler measures and also share other more complex code
      between themselves. If you eventually want two or more of these measures
      for the same data it is better to compute them all at once. The ECL
      optimizer does an excellent job of making sure code is only executed
      once however often it is used. If you are familiar with ECL at a lower
      level, you may wish to look at the graph for the following:
      <programlisting>IMPORT ML;

TestSize := 10000000;

a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);

D := b1+b2; // This is the test data

Agg := ML.FieldAggregates(D);

Agg.Simple;
Agg.SimpleRanked;
Agg.Ranked;
Agg.Modes;
Agg.Medians;
Agg.NTiles(4);
Agg.NTileRanges(4);
Agg.Buckets(4);
Agg.BucketRanges(4)

</programlisting></para>

      <para></para>
    </sect1>

    <sect1 id="Regression_walkthrough">
      <title id="Regression_walkthrough">Regression Walk-through</title>

      <para><emphasis role="bold">Modules: Regression</emphasis></para>

      <para>The Regression module exists to perform analysis and modeling of
      the relationship between a single dependent variable Y, and one or more
      independent variables Xi (also called predictor or explanatory
      variables). Regression is called ‘Simple’ if only one independent
      variable X is used. It is called ‘Multivariate’ regression when more
      than one independent variable is used.</para>

      <para>The relationship between dependent variable and independent
      variables is expressed as a function whose form has to be specified.
      Regression is called ‘Linear Regression’ if the function that defines
      the relationship is linear. For example, a Simple Linear Regression
      model expresses relationship between dependent variable Y and single
      independent variable X as a linear function:</para>

      <para>Y = β0 + β1X</para>

      <para>This function represents a line with parameters β= (β0,
      β1).</para>

      <sect2>
        <title id="Polynomial">Polynomial</title>

        <para>The Polynomial regression expresses (ie models) relationship
        between dependent variable Y and a single independent variable X with
        polynomial function of the following form:</para>

        <para>Y = β0 + β1*LogX+ β2*X+ β3*X*LogX+ β4*X2+ β5*X2*LogX+
        β6*X3</para>

        <para>with parameters:</para>

        <para>β= (β0, β1, β2, β3, β4, β5, β6)</para>

        <para>Along with the other ML functions, Polynomial Regression is
        designed to work upon huge datasets; however it can be quite useful
        even on tiny ones. The following dataset captures the time taken for a
        particular ML routine to execute against a particular number of
        records. Polynomial Regression models relationship between the number
        of records (X=Recs), and the execution time (Y=Time) and produces β
        values, which represent how much every component of the polynomial
        function, (constant, Log, XLogX, …) contributes to the execution
        time.</para>

        <para><programlisting>IMPORT ML;
R := RECORD
       INTEGER        rid;
       INTEGER        Recs;
       REAL           Time;
END;
d := DATASET([{1,50000,1.00},{2,500000,2.29}, {3,5000000,16.15},
               {4,25000000,80.2},{5,50000000,163},{6,100000000,316},
               {7,10,0.83},{8,1500000,5.63}],R);
ML.ToField(d,flds);
P := ML.Regression.Poly(flds(number=1),flds(number=2));
P.Beta;
P.RSquared
</programlisting>The Polynomial regression is configured by default to
        calculate parameters β= (β0, β1, β2, β3, β4, β5, β6). This can be
        changed to any number smaller than 6.</para>

        <para>For example, to configure polynomial regression to use the
        polynomial function Y = β0 + β1*LogX+ β2*X to model relationship
        between X and Y, the code above would need to be changed as
        follows:</para>

        <para><programlisting>P := ML.Regression.Poly(flds(number=1),flds(number=2), 2); </programlisting></para>

        <para>The third parameter ‘2’ has to be used to override the default
        value ‘6’.</para>
      </sect2>

      <sect2>
        <title id="OLS">OLS</title>

        <para>The Ordinary Least Squares (OLS) regression is a linear
        regression model that calculates parameters β using the method of
        least squares to minimize the distance between measured and predicted
        values of the dependent variable Y. The following example,
        demonstrates how it might be used:</para>

        <para><programlisting>IMPORT ML;

value_record := RECORD
                unsigned    rid;
                unsigned    age;
                real        height;
  END;

d := DATASET([{1,18,76.1}, {2,19,77}, {3,20,78.1},
            {4,21,78.2}, {5,22,78.8}, {6,23,79.7},
            {7,24,79.9}, {8,25,81.1}, {9,26,81.2},
            {10,27,81.8},{11,28,82.8}, {12,29,83.5}]
            ,value_record);

ML.ToField(d,o);

X := O(Number =1); // Pull out the age
Y := O(Number =2); // Pull out the height
Reg := ML.Regression.OLS(X,Y);
B := Reg.Beta();
B;                         
Reg.RSquared;
Reg.Anova;
</programlisting>In this example we have a dataset that contains 12 records
        with the age in months and a mean height in centimeters for children
        of that age. We use the OLS regression to find parameters β of the
        linear function, ie the line that represents (models) the relationship
        between child age and height. Once we get parameters β, we can use
        them to predict the height of a child whose age is not listed in the
        dataset.</para>

        <para>For example, the mean height of a 30 month old child can be
        predicted using the following formula:</para>

        <para>Height = β0 + 30* β1.</para>

        <para>How well does this function (ie regression model) fit the
        data?</para>

        <para>One measure of goodness of fit is the R-squared. The range of
        R-squared is [0,1], and values closer to 1 indicate better fit.</para>

        <para>For Simple Linear regression the R-squared represents a square
        of correlation between X and Y.</para>

        <para>Analysis of Variance (ANOVA) provides information about the
        level of variability within a regression model, and that information
        can be used as a basis for tests of significance. Anova returns a
        table with the following values for the model, error and total, and
        the model statistic called “F”:</para>

        <itemizedlist>
          <listitem>
            <para>sum of squares (SS)</para>
          </listitem>

          <listitem>
            <para>degrees of freedom (DF)</para>
          </listitem>

          <listitem>
            <para>mean square (MS)</para>
          </listitem>
        </itemizedlist>

        <para>The OLS regression can be configured to calculate parameters β
        using either the LU matrix decomposition or Cholesky matrix
        decomposition. Cholesky matrix decomposition is used as a default if
        no parameters are passed into the beta function. This is how the
        example above has been coded. The default matrix decomposition can be
        changed to the LU decomposition as follows: <programlisting>B := Reg.Beta(Reg.MDM.LU);</programlisting></para>
      </sect2>
    </sect1>
  </chapter>

  <chapter id="ML_modules">
    <title id="ML_modules">The ML Modules</title>

    <para>Each ML module focuses on a specific type of algorithm and contains
    a number of routines. The functionality of each routine is also
    described.</para>

    <para>Performance statistics are provided for some routines. These were
    carried out on a 10 node cluster and are for comparison purposes
    only.</para>

    <sect1 id="Associations">
      <title id="Associations">Associations (ML.Associate)</title>

      <para>Use this module to perform frequent pattern matching on the
      underlying data, as follows:</para>

      <table>
        <title></title>

        <tgroup cols="2">
          <colspec align="center" />

          <thead>
            <row>
              <entry align="center">Routine</entry>

              <entry align="center">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Apriori1,Apriori2,Apriori3</entry>

              <entry>Uses ‘old school’ brute force and speed approach to
              produce patterns of up to 3 items which appear together with a
              particular degree of support.</entry>
            </row>

            <row>
              <entry>AprioriN</entry>

              <entry>Uses ‘new school’ techniques to find all patterns of up
              to N items that appear together with a particular degree of
              support.</entry>
            </row>

            <row>
              <entry>EclatN</entry>

              <entry>Uses the ‘eclat’ technique to construct a result
              identical to AprioriN.</entry>
            </row>

            <row>
              <entry>Rules</entry>

              <entry>Uses patterns generated by AprioriN or EclatN to answer
              the question: “given a group of M items exists; what is the
              M+1th most likely to be”.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>The following performance statistics of these routines were
      observed using a 10 node cluster:</para>

      <informaltable>
        <tgroup cols="4">
          <colspec align="center" />

          <thead>
            <row>
              <entry align="center">Routine</entry>

              <entry align="center">Description</entry>

              <entry align="center">Result</entry>

              <entry align="center">Expected Order</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Apriori1</entry>

              <entry>On 140M words</entry>

              <entry>47 seconds</entry>

              <entry>NlgN</entry>
            </row>

            <row>
              <entry>Aprior1</entry>

              <entry>On 197M words</entry>

              <entry>91 seconds</entry>

              <entry>NlgN</entry>
            </row>

            <row>
              <entry>Apiori2</entry>

              <entry>On 140M words, producing 2.6K pairs</entry>

              <entry>325 seconds</entry>

              <entry>(N/k)^2.MLg(N) where k is proportion of ‘buckets’ average
              item is in. (Using terms in 5-10% of buckets)</entry>
            </row>

            <row>
              <entry>Apiori2</entry>

              <entry>On 193M words (using .1-&gt;1% buckets) – producing 4.4M
              pairs</entry>

              <entry>21 minutes</entry>

              <entry></entry>
            </row>

            <row>
              <entry>Apiori2</entry>

              <entry>On 19M words (10% sample) – producing 4.1M pairs</entry>

              <entry>2 minutes</entry>

              <entry></entry>
            </row>

            <row>
              <entry>Apiori3</entry>

              <entry>On 140M words (terms in 5-10% buckets)</entry>

              <entry>Exploded</entry>

              <entry></entry>
            </row>

            <row>
              <entry>Apiori3</entry>

              <entry>1.9M words (1% sample of .1-1 buckets) – (172K possible 3
              groups) – 3.6B intermediate results – 22337 eventual
              results</entry>

              <entry>73 minutes</entry>

              <entry></entry>
            </row>

            <row>
              <entry>Apiori3</entry>

              <entry>On 1.9M words with new ,LOOKUP optimization</entry>

              <entry>42 minutes</entry>

              <entry></entry>
            </row>

            <row>
              <entry>EE3</entry>

              <entry>On 1.9M words (1% sample of .1-1 buckets) – 22337
              eventual results</entry>

              <entry>3 minutes</entry>

              <entry></entry>
            </row>

            <row>
              <entry>EE10</entry>

              <entry>On 1.9M words</entry>

              <entry>Locks</entry>

              <entry></entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para>The following is an example of the use of the Apriori
      routine:</para>

      <para><programlisting>IMPORT ML;
IMPORT ML.Docs AS Docs;

d11 := DATASET([{'One of the wonderful things about tiggers is tiggers are wonderful 
                 things'},
                {'It is a little scarey the drivel that enters one's mind when given the 
                 task of entering random text'},
                {'I almost quoted Oscar Wilde; but I considered that I had gotten a little 
                 too silly already!'},
                {'I would hate to have quoted silly people!'},
                {'Oscar Wilde is often quoted'},
                {'In Hertford, Hereford and Hampshire Hurricanes hardly ever happen'},
                {'It is a far, far better thing that I do, than I have ever done'}],
                {string r});
d00 := DATASET([{'aa bb cc dd ee'},{'bb cc dd ee ff gg hh ii'},{'bb cc dd ee ff gg hh ii'},
                {'dd ee ff'},{'bb dd ee'}],{string r});

d := d11;

d1 := PROJECT(d,TRANSFORM(Docs.Types.Raw,SELF.Txt := LEFT.r));

d2 := Docs.Tokenize.Enumerate(d1);

d3 := Docs.Tokenize.Clean(d2);

d4 := Docs.Tokenize.Split(d3);                                      


lex := Docs.Tokenize.Lexicon(d4);


o1 := Docs.Tokenize.ToO(d4,lex);
o2 := Docs.Trans(O1).WordBag;

lex;
ForAssoc := PROJECT( o2, TRANSFORM(ML.Types.ItemElement,SELF.id := LEFT.id,
SELF.value := LEFT.word ));
ForAssoc;
ML.Associate(ForAssoc,2).Apriori1;
ML.Associate(ForAssoc,2).Apriori2;
ML.Associate(ForAssoc,2).Apriori3;
ML.Associate(ForAssoc,2).AprioriN(40);
</programlisting></para>

      <para></para>
    </sect1>

    <sect1 id="Classify">
      <title id="Classify">Classify (ML.Classify)</title>

      <para>Use this module to tackle the problem, “can I predict this
      dependent variable based upon these independent ones?”. The following
      routines are provided:</para>

      <informaltable>
        <tgroup cols="2">
          <colspec align="center" />

          <thead>
            <row>
              <entry align="center">Routine</entry>

              <entry align="center">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>BuildNaiveBayes</entry>

              <entry>Builds a Bayes model for one or more dependent
              variables.</entry>
            </row>

            <row>
              <entry>NaiveBayes</entry>

              <entry>Executes one or more Bayes models against an underlying
              dataset to compute dependent variables.</entry>
            </row>

            <row>
              <entry>TestNaiveBayes</entry>

              <entry>Generates a module containing four different measures of
              how well the classification models are doing. This calculation
              is based on the Bayes model and set of independent variables
              with outcome data supplied.</entry>
            </row>

            <row>
              <entry>BuildPerceptron</entry>

              <entry>Builds a perceptron for multiple dependent (Boolean)
              variables.</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para></para>
    </sect1>

    <sect1 id="Cluster">
      <title id="Cluster">Cluster (ML.Cluster)</title>

      <para>This module is used to perform the clustering of a collection of
      records containing fields. The following routines are provided:</para>

      <para><informaltable>
          <tgroup cols="2">
            <colspec align="center" />

            <thead>
              <row>
                <entry align="center">Routine</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>DF</entry>

                <entry>A submodule used to perform various distance metrics
                upon two records. Currently, the following are provided:
                <itemizedlist>
                    <listitem>
                      <para>Euclidean</para>
                    </listitem>

                    <listitem>
                      <para>Euclidean Squared</para>
                    </listitem>

                    <listitem>
                      <para>Manhattan</para>
                    </listitem>

                    <listitem>
                      <para>Cosine</para>
                    </listitem>

                    <listitem>
                      <para>Tanimoto</para>
                    </listitem>
                  </itemizedlist>Quick variants exist for Euclidean and
                Euclidean Squared which are much faster on sparse data
                PROVIDED you are willing to accept no distance if there are no
                dimensions along which the vectors touch.</entry>
              </row>

              <row>
                <entry>Distances</entry>

                <entry>The engine to actually compute the distance matrix (as
                a matrix).</entry>
              </row>

              <row>
                <entry>Closest</entry>

                <entry>Takes a set of distances and returns the closest
                centroid for each row.</entry>
              </row>

              <row>
                <entry>KMeans</entry>

                <entry>Performs KMeans clustering for a specified number of
                iterations.</entry>
              </row>

              <row>
                <entry>AggloN</entry>

                <entry>Performs Agglomerative (Hierarchical) clustering. The
                results include the cluster assignments, remaining distances
                between clusters and even the dentrogram.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <sect2 id="KMeans">
        <title id="kmeans_module">KMeans</title>

        <para>The KMeans module performs K-Means clustering on a static
        primary data set and a pre-determined set of centroids. This is an
        iterative two-step process. For each iteration, every data entity is
        assigned to the centroid that is closest to it, and then the centroid
        locations are re-assigned to the mean position of all the data
        entities assigned to them.</para>

        <para>The user passes in the two datasets and the maximum number of
        iterations to perform if convergence is not achieved. Optionally, the
        user may also define a convergence threshold (default=0.0) and the
        distance function to be used during calculations (default is
        Euclidean).</para>

        <para>The primary output from the KMeans module is “Result()”, which
        returns the centroid dataset with their new locations. “Convergence”
        will indicate the number of iterations performed, which will never be
        greater than the maximum number passed in to the function. There are
        also a number other useful return values that enable the user to
        analyze centroid movement, which are included in the following
        example.</para>

        <para><programlisting>IMPORT ML;

lMatrix:={UNSIGNED id;REAL x;REAL y;};

// Simple two-dimensional set of numbers between 0-10
dDocumentMatrix:=DATASET([
  {1,2.4639,7.8579},
  {2,0.5573,9.4681},
  {3,4.6054,8.4723},
  {4,1.24,7.3835},
  {5,7.8253,4.8205},
  {6,3.0965,3.4085},
  {7,8.8631,1.4446},
  {8,5.8085,9.1887},
  {9,1.3813,0.515},
  {10,2.7123,9.2429},
  {11,6.786,4.9368},
  {12,9.0227,5.8075},
  {13,8.55,0.074},
  {14,1.7074,3.9685},
  {15,5.7943,3.4692},
  {16,8.3931,8.5849},
  {17,4.7333,5.3947},
  {18,1.069,3.2497},
  {19,9.3669,7.7855},
  {20,2.3341,8.5196}
],lMatrix);

// Arbitrary but non-symmetric centroid starting points
dCentroidMatrix:=DATASET([
  {1,1,1},
  {2,2,2},
  {3,3,3},
  {4,4,4}
],lMatrix);

// Convert the above matrices into the NumericField format
ML.ToField(dDocumentMatrix,dDocuments);
ML.ToField(dCentroidMatrix,dCentroids);

// Set up KMeans with a maximum of 30 iterations and .3 as a convergence threshold
KMeans:=ML.Cluster.KMeans(dDocuments,dCentroids,30,.3);

// The table that contains the results of each iteration
KMeans.Allresults;
// The number of iterations it took to converge
KMeans.Convergence;
// The results of iteration 3
KMeans.Result(3);
// The distance every centroid travelled across each axis from iterations 3 to 5
KMeans.Delta(3,5);
// The total distance the centroids travelled on each axis
KMeans.Delta(0);
// The straight-line distance travelled by each centroid from iterations 3 to 5
KMeans.DistanceDelta(3,5);
// The total straight-line distance each centroid travelled 
KMeans.DistanceDelta(0);
// The distance travelled by each centroid during the last iteration.
KMeans.DistanceDelta();
</programlisting></para>
      </sect2>

      <sect2>
        <title id="AggloN_function">AggloN</title>

        <para>The AggloN function performs Agglomerative clustering on a
        static document set. This is a bottom-up approach whereby close pairs
        are found and linked, and then treated as a single entity during the
        next iteration. Allowed to run fully, the result will be a single tree
        structure with multiple branches and sub-branches.</para>

        <para>The dataset and the maximum number of iterations are the two
        required parameters for this function. The user may also optionally
        specify the distance formula which defaults to Euclidean. The
        following is an example using the AggloN routine (using the same
        dDocuments constructed in the above example):</para>

        <para><programlisting>// Set up Agglomerative clustering with 4 iterations
AggloN:=ML.Cluster.AggloN(dDocuments,4);

// Table with nested sets of numbers delineating the tree after the specified
// number of iterations
AggloN.Dendrogram;
// Table containing the distances between each pair of clusters
AggloN.Distances;
// List of entities and the cluster they are assigned to (the cluster ID will always
// be the lowest ID value of the entities that comprise the cluster)
AggloN.Clusters;
</programlisting></para>
      </sect2>

      <sect2 id="Distances">
        <title id="Distances">Distances</title>

        <para>This is the engine for calculating the distances between every
        entity in one dataset to every entity in a second dataset, which are
        the two required parameters. If the first dataset is also passed as
        the second one, then a self-join is performed. Otherwise, it is
        assumed that the IDs for the second dataset do not intersect with
        those from the first one. The default formula used for distance
        calculation is Euclidean, but that may be changed to any of the other
        distance functions available in the DF module.</para>

        <para>The code for the Distances function is fairly complex in an
        effort to facilitate both flexibility and efficiency. The user is able
        to calculate distances in three modes:</para>

        <informaltable>
          <tgroup cols="2">
            <tbody>
              <row>
                <entry>Dense</entry>

                <entry>All calculations are performed, which for a self-join
                is an N^2 problem.</entry>
              </row>

              <row>
                <entry>Summary Join</entry>

                <entry>Highly efficient but not as accurate, this process only
                calculates distance between entities that share at least one
                dimension.</entry>
              </row>

              <row>
                <entry>Background</entry>

                <entry>Fully accurate and highly efficient, assuming a sparse
                matrix. This process takes advantage of the assumption that
                only a small number of dimensions are shared between any two
                pairs of entities.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable>

        <para>This is fully accurate and highly efficient, assuming a sparse
        matrix. This process takes advantage of the assumption that only a
        small number of dimensions are shared between any two pairs of
        entities.</para>
      </sect2>
    </sect1>

    <sect1 id="Correlations">
      <title id="Correlations">Correlations (ML.Correlate)</title>

      <para>Use this module to calculate the degree of correlation between
      every pair of fields provided, using the following routines:</para>

      <para><informaltable>
          <tgroup cols="2">
            <colspec align="center" />

            <thead>
              <row>
                <entry align="center">Routine</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Simple</entry>

                <entry>Pearson and Spearman correlation co-efficients for
                every pair of fields.</entry>
              </row>

              <row>
                <entry>Kendal</entry>

                <entry>Kendal’s Tau for every pair of fields.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>The correlation tables expose multicollinearity issues and enable
      you to look at predicted versus original to expose heteroscedasticity
      issues.</para>
    </sect1>

    <sect1 id="Discretize">
      <title id="Discretize">Discretize (ML.Discretize)</title>

      <para>This module provides a suite of routines which allow a datastream
      with continuous real elements to be turned into a stream with discrete
      (integer) elements. The Discretize module currently supports three
      methods of discretization, ByRounding, ByBucketing and ByTiling. These
      method can be used by hand for reasons of simplicity and control.</para>

      <para>In addition, it is possible to turn them into an instruction
      stream for an 'engine' to execute. Using them in this way allows the
      discretization strategy to be in meta-data. Use the Do definition to
      construct the meta-data fragment, which will then perform the
      discretization. This enables the automatic generation of strategies and
      even iterates over the modeling process with different discretization
      strategies which can be programmatically generated.</para>

      <para><informaltable>
          <tgroup cols="2">
            <colspec align="center" />

            <thead>
              <row>
                <entry align="center">Routine</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>ByRounding</entry>

                <entry>Using scale and delta.</entry>
              </row>

              <row>
                <entry>ByBucketing</entry>

                <entry>Split the range evenly and distribute the values
                potentially unevenly.</entry>
              </row>

              <row>
                <entry>ByTiling</entry>

                <entry>Split the values evenly and have an uneven
                range.</entry>
              </row>

              <row>
                <entry>Do</entry>

                <entry>Constructs a meta-data fragment which will then perform
                the discretization.</entry>
              </row>

              <row>
                <entry>Mentioned in email but may not be implemented</entry>

                <entry></entry>
              </row>

              <row>
                <entry>Naive Bayes</entry>

                <entry>Works on data that has been discretiized. Requires two
                datasets, a set of independent variables and a set of classes.
                It allows multiple classifiers to be built simultaneously.
                First the classifier model is built then the model is executed
                against a dataset and produces an outcome.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>The following is an example of the use of the Naive Bayes
      routine:</para>

      <para><programlisting>import ml;

value_record := RECORD
        unsigned rid;
real height;
        real weight;
        real age;
        integer1 species;
        integer1 gender; // 0 = unknown, 1 = male, 2 = female
END;

d := dataset([{1,5*12+7,156*16,43,1,1},
                                              {2,5*12+7,128*16,31,1,2},
                                              {3,5*12+9,135*16,15,1,1},
                                              {4,5*12+7,145*16,14,1,1},
                                              {5,5*12-2,80*16,9,1,1},
                                              {6,4*12+8,72*16,8,1,1},
                                              {7,8,32,2.5,2,2},
                                              {8,6.5,28,2,2,2},
                                              {9,6.5,28,2,2,2},
                                              {10,6.5,21,2,2,1},
                                              {11,4,15,1,2,0},
                                              {12,3,10.5,1,2,0},
                                              {13,2.5,3,0.8,2,0},
                                              {14,1,1,0.4,2,0}
                                              ]
                                              ,value_record);
  
// Turn into regular NumericField file (with continuous variables)
ml.ToField(d,o);

// Hand-code the discretization of some of the variables
disc := ML.Discretize.ByBucketing(o(Number IN [2,3]),4)+ML.Discretize.ByTiling(o(Number IN
[1]),6)+ML.Discretize.ByRounding(o(Number=4));

// Create instructions to be executed
inst := 
ML.Discretize.i_ByBucketing([2,3],4)+ML.Discretize.i_ByTiling([1],6)+
ML.Discretize.i_ByRounding([4,5]);

// Execute the instructions
done := ML.Discretize.Do(o,inst);

//m1 := ML.Classify.BuildPerceptron(done(Number&lt;=3),done(Number&gt;=4));
//m1

m1 := ML.Classify.BuildNaiveBayes(done(Number&lt;=3),done(Number&gt;=4));
m1;

Test := ML.Classify.TestNaiveBayes(done(Number&lt;=3),done(Number&gt;=4),m1);
Test.Raw;
Test.CrossAssignments;
Test.PrecisionByClass;
Test.Headline;
</programlisting></para>

      <para>The following is an example of the use of the discretize
      routines:</para>

      <para><programlisting>import ml;

value_record := RECORD
                unsigned rid;
  real height;
                real weight;
                real age;
                integer1 species;
  END;

d := dataset([{1,5*12+7,156*16,43,1},
                                                            {2,5*12+7,128*16,31,1},
                                                            {3,5*12+9,135*16,15,1},
                                                            {4,5*12+7,145*16,14,1},
                                                            {5,5*12-2,80*16,9,1},
                                                            {6,4*12+8,72*16,8,1},
                                                            {7,8,32,2.5,2},
                                                            {8,6.5,28,2,2},
                                                            {9,6.5,28,2,2},
                                                            {10,6.5,21,2,2},
                                                            {11,4,15,1,2},
                                                            {12,3,10.5,1,2},
                                                            {13,2.5,3,0.8,2},
                                                            {14,1,1,0.4,2}             
                                                                             ]
                                                                             value_record);

// Turn into regular NumericField file (with continuous variables)
ml.macPivot(d,o);

// Hand-code the discretization of some of the variables
disc := ML.Discretize.ByBucketing(o(Number = 3),4)+ML.Discretize.ByTiling(o
(Number IN [1,2]),4)+ML.Discretize.ByRounding(o(Number=4));
disc;

// Create instructions to be executed
inst := ML.Discretize.i_ByBucketing([3],4)+ML.Discretize.i_ByTiling([1,2],4)
+ML.Discretize.i_ByRounding([4]);

// Execute the instructions
done := ML.Discretize.Do(o,inst);
done;
</programlisting></para>
    </sect1>

    <sect1 id="Distribution">
      <title id="Distribution">Distribution (ML.Distribution)</title>

      <para>The distribution module exists to provide code to generate
      distribution tables and ‘random’ data according to a particular
      distribution. Each distribution is a ‘module’ that takes a collection of
      parameters and then implements a common interface. Other than the
      ‘natural’ mathematics of each distribution the implementation adds the
      notion of ranges (or NRanges). Essentially this means that the codomain
      (range) of the distribution is split into NRanges; this can be thought
      of as a degree of granularity. For discrete distributions this is
      naturally the number of results that can be produced. For continuous
      distributions you should think of the distribution curve as being
      approximated by NRanges straight lines. The maximum granularity
      currently supported in 1M ranges (after that the numeric pain of
      retaining precision is too nasty).</para>

      <para>The interface to the distribution provides:</para>

      <para><programlisting>  EXPORT t_FieldReal Density(t_FieldReal RH)
// The probability density function at point RH

EXPORT t_FieldReal Cumulative(t_FieldReal RH)
// The cumulative probability function from – infinity[1] up to RH

  EXPORT DensityV() 
// A vector providing the probability density function at each range point – 
   this is approximately equal to the ‘distribution tables’ that might be published 
   in various books

EXPORT CumulativeV()
// A vector providing the cumulative probability density function at each range point 
   – again roughly equal to ‘cumulative distribution tables’ as published in the back of 
   statistics books

EXPORT Ntile(Pcnt
//provides the value from the underlying domain that corresponds to the given percentile. 
  Thus .Ntile(99) gives the value beneath which 99% of all should observations will fall.
</programlisting></para>

      <para><informaltable>
          <tgroup cols="2">
            <colspec align="center" />

            <thead>
              <row>
                <entry align="center">Routine</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Uniform (Low, High, NRanges.)</entry>

                <entry>Specifies that any (continuous) value between Low and
                High is equally likely to occur.</entry>
              </row>

              <row>
                <entry>StudentT (degrees-of-freedon, NRanges)</entry>

                <entry>Specifies the degrees of freedom for a Student-T
                distribution. Note this module also exports InvDensity to
                provide the ‘t’ value that gives a particular density value
                (can be useful as the ‘tail’ of a t distribution is often
                interesting).</entry>
              </row>

              <row>
                <entry>Normal (Mean, Standard Deviation, NRanges)</entry>

                <entry>Implements a normal distribution (bell curve) – with
                mean ‘mean’ and standard deviation as specified – approximate
                by NRanges straight lines.</entry>
              </row>

              <row>
                <entry>Exponential (Lamda, NRanges))</entry>

                <entry>Implements the exponential (sometimes called negative
                exponential) distribution.</entry>
              </row>

              <row>
                <entry>Binomial (p, NRanges)</entry>

                <entry>Gives the distribution for the chances of getting ‘k’
                successful events in Nranges-1 trials where the chances of
                success in one trial is ‘p’.</entry>
              </row>

              <row>
                <entry>NegBinomial (p, failures, NRanges)</entry>

                <entry>Gives the distribution for the chances of getting ‘k’
                successful events before ‘failures’ number of failures occurs
                (maximum total trials = nranges). The geometric distribution
                can be obtained by setting failures = 1.</entry>
              </row>

              <row>
                <entry>Poisson (Lamda, NRanges)</entry>

                <entry>For a poisson distribution the mean and variance are
                both provided by Lamda. It is a discrete distribution (will
                only produce integral values). Thus if NRanges is (say) 100
                then the probability function for Poisson will be computed for
                values of 0 to NRanges-1.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>Most people do not really encounter StudentT as a distribution,
      rather they encounter the t-test. You can perform a t-test using the t
      distribution using the NTile capability. Thus the value for a
      single-tailed t-test with 3 degrees of freedom at the 99% confidence
      level can be obtained using:<programlisting>a := ML.Distribution.StudentT(3,10000);
a.NTile(99); // Single tail
a.NTile(99.5); // Double tail
</programlisting></para>

      <note>
        <para>The Cauchy distribution can be obtained by setting v = 1.</para>
      </note>

      <para>This module also exports:</para>

      <para><programlisting>GenData(NRecords,Distribution,FieldNumber)</programlisting>This
      allows N records to be generated each with a given field-number and with
      random values distributed according to the specified distribution. If a
      ‘single stream’ of random numbers is required then FieldNumber may be
      set to 1. It is provided to allow ‘random records’ with multiple fields
      to be produced as in the following usage example:<programlisting>IMPORT * FROM ML;
//a := ML.Distribution.Normal(4,5,10000);
a := ML.Distribution.Poisson(40,100);
//a := ML.Distribution.Uniform(0,100,10000);
a.Cumulative(5);
choosen(a.DensityV(),1000);
choosen(a.CumulativeV(),1000);
b := ML.Distribution.GenData(200000,a);
ave(b,value);
variance(b,value)
</programlisting></para>

      <para>The following performance timings were done generating 3 fields,
      two normal and one poisson:</para>

      <informaltable>
        <tgroup cols="2">
          <colspec align="center" />

          <thead>
            <row>
              <entry align="center">Records</entry>

              <entry align="center">Time (secs)</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>50K</entry>

              <entry>1.00</entry>
            </row>

            <row>
              <entry>500K</entry>

              <entry>2.29</entry>
            </row>

            <row>
              <entry>5M</entry>

              <entry>16.15</entry>
            </row>

            <row>
              <entry>25M</entry>

              <entry>80.2</entry>
            </row>

            <row>
              <entry>50M</entry>

              <entry>163</entry>
            </row>

            <row>
              <entry>100M</entry>

              <entry>316</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para></para>
    </sect1>

    <sect1 id="Field_Aggregates_Module">
      <title id="Field_Aggregates_Module">FieldAggregates
      (ML.FieldAggregates)</title>

      <para>This module works on the field elements provided. It performs the
      tasks shown below for ALL the fields at once:</para>

      <para><informaltable>
          <tgroup cols="2">
            <colspec align="center" />

            <thead>
              <row>
                <entry align="center">Routine</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Simple stats</entry>

                <entry>Mean, Variance, Standard Deviation, Max, Min, Count,
                Sums, etc</entry>
              </row>

              <row>
                <entry>Medians</entry>

                <entry>Provides the median elements.</entry>
              </row>

              <row>
                <entry>Modes</entry>

                <entry>Provides the modal value(s) of each field.</entry>
              </row>

              <row>
                <entry>Cardinality</entry>

                <entry>Provides the cardinality of each field.</entry>
              </row>

              <row>
                <entry>Buckets/Bucket Ranges</entry>

                <entry>Divides the domain of each field evenly and then counts
                the number of elements falling into each range. Can be used to
                graphically plot the distribution of a field</entry>
              </row>

              <row>
                <entry>SimpleRanked</entry>

                <entry>Lists the ranking of the elements from the smallest to
                the largest.</entry>
              </row>

              <row>
                <entry>Ranked</entry>

                <entry>Adjusts the simple ranking to allow for repeated
                elements (each repeated element gets a rank which is the mean
                of the ranks provided to each element individually)</entry>
              </row>

              <row>
                <entry>NTiles/NTileRanges</entry>

                <entry>Think of this as giving the percentile rank of each
                element – except you get to pick if it is percentiles (N=100)
                or some other gradation</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>
    </sect1>

    <sect1 id="Regression">
      <title id="Regression">Regression (ML.Regression)</title>

      <para><informaltable>
          <tgroup cols="2">
            <colspec align="center" />

            <thead>
              <row>
                <entry align="center">Routine</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>OLS</entry>

                <entry>Takes a set of independent and dependent variables and
                exports a module that publishes an ordinary least squares
                linear regression of the independent variables to produce the
                dependent ones (it can perform multiple regressions at
                once).</entry>
              </row>

              <row>
                <entry>Poly</entry>

                <entry>Performs a polynomial regression for a single
                independent variable with a single dependent target.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>The OLS routine can also return R^2 and Anova tables based upon
      the regression.</para>

      <para>Currently, Poly handles polynomials up to X^3 and includes
      logarithms. Along with the other ML functions Poly is designed to work
      upon huge datasets; however it can be quite useful even on tiny ones.
      The following dataset captures the time taken for a particular ML
      routine to execute against a particular number of records. It then
      produces a table showing the expected running time for any number of
      records that are entered:<programlisting>IMPORT ML;

R := RECORD
                INTEGER rid;
  INTEGER Recs;
                REAL Time;
                END;      
d := DATASET([{1,50000,1.00},{2,500000,2.29},         {3,5000000,16.15},{4,25000000,80.2},
                                                      {5,50000000,163},{6,100000000,316},
                                                      {7,10,0.83},{8,1500000,5.63}],R);

ML.ToField(d,flds);

P := ML.Regression.Poly(flds(number=1),flds(number=2),4);
P.Beta;
P.RSquared
</programlisting></para>

      <para>The R squared is a measure of goodness of fit.</para>
    </sect1>
  </chapter>

  <chapter id="ML_with_documents">
    <title id="ML_with_documents">Using ML with documents</title>

    <sect1 id="Docs_module">
      <title id="ML.Doc">The Docs Module (ML.Doc)</title>

      <para>The ML.Docs module provides a number of routines which can be used
      to pre-process text and make it more suitable for further processing.
      The following routines are provided:</para>

      <informaltable>
        <tgroup cols="3">
          <colspec align="center" />

          <thead>
            <row>
              <entry align="center">Routine</entry>

              <entry align="center">Sub-Routine</entry>

              <entry align="center">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Tokenize</entry>

              <entry></entry>

              <entry>Routines which turn raw text into a clean processing
              format.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Enumerate</entry>

              <entry>Applies record numbers to the text for later
              tracking.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Clean</entry>

              <entry>Removes lots of nasty punctuation and some other things
              such as possessives.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Split</entry>

              <entry>Turns the document into a token (or word) stream.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Lexicon</entry>

              <entry>Constructs a dictionary (with various statistics) on the
              underlying documents.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>ToO/FromO</entry>

              <entry>Uses the lexicon to turn the word stream to and from an
              optimized token processing format.</entry>
            </row>

            <row>
              <entry>Trans</entry>

              <entry></entry>

              <entry>Performs various data transformations on an optimized
              document stream.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>WordBag</entry>

              <entry>Turns every document into a wordbag, by removing (and
              counting) multiple occurrences of a word within a
              document.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>WordsCounted</entry>

              <entry>Annotates every word in a document with the total number
              of times that word occurs in the document, distributing tfdi
              information for further (faster) processing.</entry>
            </row>

            <row>
              <entry>CoLocation</entry>

              <entry></entry>

              <entry>Functions related to the co-occurrence of NGrams and
              their constituents.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>AllNGrams</entry>

              <entry>A breakdown of all n-grams within a set of raw document
              text, where n is specified by the user.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>NGrams</entry>

              <entry>Aggregate information for every unique n-gram found
              within the document corpus.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Support</entry>

              <entry>The ratio of the number of documents that contain all of
              the n-grams in the parameter set compared to the corpus
              count.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Confidence</entry>

              <entry>The ratio of the number of documents that contain all of
              the n-grams in both parameter sets compared to the number of
              documents that only contain the n-grams in the first
              set.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Lift</entry>

              <entry>The ratio of observed support of two sets of n-grams
              compared to the support if the n-grams were independent.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Conviction</entry>

              <entry>The ratio of how often one set of n-grams exists without
              a second set given that the two are independent.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>SubGrams</entry>

              <entry>Compares the product of the document frequencies of all
              constituent unigrams to the n-gram they comprise.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>SplitCompare</entry>

              <entry>Compares the document frequency of every n-gram to the
              individual frequencies of a member unigram and the remaining
              n-1-gram.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>ShowPhrase</entry>

              <entry>If lexical substitution was using in constructing the
              AllNGrams dataset, this function will re-constitute the text
              associated with their corresponding numbers within an
              n-gram.</entry>
            </row>

            <row>
              <entry>PorterStem</entry>

              <entry></entry>

              <entry>Standard Porter stemmer in ECL.</entry>
            </row>

            <row>
              <entry>PorterStemC</entry>

              <entry></entry>

              <entry>Standard Porter stemmer in C (faster).</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para></para>
    </sect1>

    <sect1>
      <title id="Doc_module_usage">Typical usage of the Docs module</title>

      <para>The following code demonstrates how the docs module may be used
      and show the result at each stage:</para>

      <para><programlisting>IMPORT ML;
IMPORT ML.Docs AS Docs;

d := DATASET([{'One of the wonderful things about tiggers is tiggers are wonderful things'},
                                  {'It is a little scary the drivel that enters one's mind 
                                    when given the task of entering random text'},
                                  {'I almost quoted Oscar Wilde; but I considered that I 
                                    had gotten a little too silly already!'},
                                  {'In Hertford, Hereford and Hampshire Hurricanes hardly 
                                    ever happen'},
                                  {'It is a far, far better thing that I do, than I have 
                                    ever done'}],{string r});

d1 := PROJECT(d,TRANSFORM(Docs.Types.Raw,SELF.Txt := LEFT.r));

d1;

d2 := Docs.Tokenize.Enumerate(d1);

d2;

d3 := Docs.Tokenize.Clean(d2);

d3;

d4 := Docs.Tokenize.Split(d3); 

d4;

lex := Docs.Tokenize.Lexicon(d4);

lex;

o1 := Docs.Tokenize.ToO(d4,lex);
o1;

Docs.Trans(o1).WordBag;
Docs.Trans(o1).WordsCounted;

o2 := Docs.Tokenize.FromO(o1,lex);
o2;
</programlisting></para>
    </sect1>

    <sect1>
      <title id="Performance_statistics">Performance Statistics</title>

      <para>The following performance statistics of these routines were
      observed using a 10 node cluster:</para>

      <informaltable>
        <tgroup cols="4">
          <colspec align="center" />

          <thead>
            <row>
              <entry align="center">Routine</entry>

              <entry align="center">Description</entry>

              <entry align="center">Result</entry>

              <entry align="center">Expected Order</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Clean and Split</entry>

              <entry>22m documents producing 1.5B words</entry>

              <entry>60 minutes</entry>

              <entry>Linear</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>1.5B words producing 6.4M entries</entry>

              <entry>40 minutes</entry>

              <entry>NlgN</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>Creating 'working' entries from 1.5B words and the full
              6.4m entry lexicon.</entry>

              <entry>46 minutes</entry>

              <entry>NLgN</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>Creating ‘working’ entries from 1.5B words and the
              ‘keyword’ lexicon, produces 140M words</entry>

              <entry>37 minutes</entry>

              <entry>NLgN</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>Creating ‘working’ entries from 1.5B words and the
              ‘keyword MkII’ lexicon, produces 240M words</entry>

              <entry>40 minutes</entry>

              <entry>NLgN</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>Creating ‘working’ entries from 1.5B words and the
              ‘keyword MkII’ lexicon, produces 240M words, using the ‘small
              lexicon’ feature</entry>

              <entry>7 minutes</entry>

              <entry>Linear</entry>
            </row>

            <row>
              <entry>Wordbag</entry>

              <entry>Create WordBags from 140M words</entry>

              <entry>114 seconds</entry>

              <entry>NlgN</entry>
            </row>

            <row>
              <entry>Wordbag</entry>

              <entry>Create WordBags from 240M-&gt;193M words</entry>

              <entry>297 seconds</entry>

              <entry>NlgN</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>
    </sect1>
  </chapter>

  <chapter>
    <title id="ML_implementation">Useful routines for ML
    implementation</title>

    <para>The following modules are provided to help with ML
    implementation:</para>

    <itemizedlist>
      <listitem>
        <para>The Utility module</para>
      </listitem>
    </itemizedlist>

    <para><itemizedlist>
        <listitem>
          <para>The Matrix module</para>
        </listitem>
      </itemizedlist></para>

    <sect1 id="Roxie-Data-Backup">
      <title id="Utility">Utility</title>

      <para>This module includes the following:</para>

      <para><informaltable>
          <tgroup cols="2">
            <colspec align="center" />

            <thead>
              <row>
                <entry align="center">Routine</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>ML.ToFieldElement/FromFieldElement</entry>

                <entry>Translates in and out of the core field-element
                datamodel.</entry>
              </row>

              <row>
                <entry>ML.Types.ToMatrix/FromMatrix</entry>

                <entry>Translates from field elements to and from
                matrices.</entry>
              </row>

              <row>
                <entry>In the email but may not be implemented...</entry>

                <entry></entry>
              </row>

              <row>
                <entry>Model comparison</entry>

                <entry></entry>
              </row>

              <row>
                <entry></entry>

                <entry></entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>
    </sect1>

    <sect1 id="Matrix_Library">
      <title id="Matrix_Library">The Matrix Library (Mat)</title>

      <para>This library is in addition to and subservient to the ML library
      modules.</para>

      <para><informaltable>
          <tgroup cols="2">
            <colspec align="center" />

            <thead>
              <row>
                <entry align="center">Routine</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Add</entry>

                <entry>Add two matrices.</entry>
              </row>

              <row>
                <entry>Choleski</entry>

                <entry>Decomposition.</entry>
              </row>

              <row>
                <entry>Det</entry>

                <entry>Determinant of a matrix.</entry>
              </row>

              <row>
                <entry>Each</entry>

                <entry>Some ‘element by element’ processing steps.</entry>
              </row>

              <row>
                <entry>Eq</entry>

                <entry>Are two matrices equal.</entry>
              </row>

              <row>
                <entry>Has</entry>

                <entry>Various matrix properties.</entry>
              </row>

              <row>
                <entry>Identity</entry>

                <entry>Construct an Identity Matrix.</entry>
              </row>

              <row>
                <entry>InsertColumn</entry>

                <entry>Add a column to a matrix.</entry>
              </row>

              <row>
                <entry>Inv</entry>

                <entry>Invert a matrix.</entry>
              </row>

              <row>
                <entry>Is</entry>

                <entry>Boolean tests for certain matrix types (Identity, Zero,
                Diagonal, Triangular etc).</entry>
              </row>

              <row>
                <entry>LU</entry>

                <entry>LU decomposition of a matrix.</entry>
              </row>

              <row>
                <entry>MU</entry>

                <entry>Matrix Universe – a number of routines to allow
                multiple matrices to exist within the same ‘file’ (or
                dataflow). Useful for iterating around loops etc.</entry>
              </row>

              <row>
                <entry>Mul</entry>

                <entry>Multiply two matrices.</entry>
              </row>

              <row>
                <entry>Pow</entry>

                <entry>Multiplies a matrix by itself N-1 * (and demo’s
                MU).</entry>
              </row>

              <row>
                <entry>RoundDelta</entry>

                <entry>Round all the elements of a matrix if they are within
                delta of an integer.</entry>
              </row>

              <row>
                <entry>Scale</entry>

                <entry>Multiply a matrix by a constant.</entry>
              </row>

              <row>
                <entry>Sub</entry>

                <entry>Subtract one matrix from another.</entry>
              </row>

              <row>
                <entry>Substitute</entry>

                <entry>Construct a matrix which is all the elements of the
                right + any elements from the left which are not in the
                right.</entry>
              </row>

              <row>
                <entry>Thin</entry>

                <entry>Make sure a matrix is fully sparse.</entry>
              </row>

              <row>
                <entry>Trans</entry>

                <entry>Construct the transpose of a matrix.</entry>
              </row>

              <row>
                <entry>Vec</entry>

                <entry>A vector library – to create vectors from matrices and
                assign vectors into matrices.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>The following examples demonstrate some of these routines:</para>

      <para><programlisting>IMPORT ML;
IMPORT ML.Mat AS Mat;
d := dataset([{1,1,1.0},{1,2,2.0},{2,1,3.0},{2,2,4.0}],Mat.Types.Element);

Mat.Sub( Mat.Scale(d,10.0), d );
Mat.Mul(d,d);
Mat.Trans(d);
</programlisting></para>
    </sect1>
  </chapter>
</book>
