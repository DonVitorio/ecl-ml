<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.1.2//EN"
"http://www.oasis-open.org/docbook/xml/4.1.2/docbookx.dtd">
<book lang="en_US">
  <bookinfo>
    <title>Machine Learning Library Reference</title>

    <mediaobject>
      <imageobject>
        <imagedata fileref="images/redswooshWithLogo3.jpg" />
      </imageobject>
    </mediaobject>

    <author>
      <surname>Boca Raton Documentation Team</surname>
    </author>

    <legalnotice>
      <para>We welcome your comments and feedback about this document via
      email to <email>docfeedback@hpccsystems.com</email> Please include
      <emphasis role="bold">Documentation Feedback</emphasis> in the subject
      line and reference the document name, page numbers, and current Version
      Number in the text of the message.</para>

      <para>LexisNexis and the Knowledge Burst logo are registered trademarks
      of Reed Elsevier Properties Inc., used under license. Other products,
      logos, and services may be trademarks or registered trademarks of their
      respective companies. All names and example data used in this manual are
      fictitious. Any similarity to actual persons, living or dead, is purely
      coincidental.</para>

      <para></para>
    </legalnotice>

    <releaseinfo>Â© 2012 HPCC Systems. All rights reserved</releaseinfo>

    <date>January 2012 Version 1.0</date>

    <corpname>HPCC Systems</corpname>

    <copyright>
      <year>2012 HPCC Systems. All rights reserved</year>
    </copyright>

    <mediaobject role="logo">
      <imageobject>
        <imagedata fileref="images/iStock_000012344803XSmall.jpg" />
      </imageobject>
    </mediaobject>
  </bookinfo>

  <chapter id="Introduction_Installation">
    <title>Introduction and Installation</title>

    <para>LexisNexis Risk Solutions is an industry leader in data content,
    data aggregation, and information services which has independently
    developed and implemented a solution for data-intensive computing called
    HPCC (High-Performance Computing Cluster).</para>

    <para>The HPCC System is designed to run on clusters, leveraging the
    resources across all nodes. However, it can also be installed on a single
    machine and/or VM for learning or test purposes. It will also run on the
    AWS Cloud.</para>

    <para>The HPCC platform also includes ECL (Enterprise Control Language)
    which is a powerful high-level, heavily-optimized, data-centric
    declarative language used for parallel data processing. The flexibility of
    the ECL language is such that any ECL code will run unmodified regardless
    of the size of the cluster being used.</para>

    <para>Instructions for installing the HPCC are available on the HPCC
    Systems website, <ulink
    url="http://hpccsystems.com/community/docs/installation-and-administration">http://hpccsystems.com/community/docs/installation-and-administration</ulink>.
    More information about running HPCC on the AWS Cloud can be found in this
    document, <emphasis>Running the HPCC System's Thor Platform within Amazon
    Web Services</emphasis>, <ulink
    url="http://hpccsystems.com/community/docs/aws-install-thor">http://hpccsystems.com/community/docs/aws-install-thor</ulink>.</para>

    <sect1 id="HPCC_Platform">
      <title>The HPCC Platform</title>

      <para>HPCC is fast, flexible and highly scalable. It can be used for any
      data-centric task and can meet the needs of any database regardless of
      size. There are two types of cluster:</para>

      <itemizedlist>
        <listitem>
          <para>The Thor cluster is used to process all data in all
          files.</para>
        </listitem>

        <listitem>
          <para>The Roxie cluster is used to search for a particular record or
          set of records.</para>
        </listitem>
      </itemizedlist>

      <para>As shown by the following diagram, the HPCC architecture also
      incorporates:</para>

      <itemizedlist>
        <listitem>
          <para>Common middle-ware components.</para>
        </listitem>

        <listitem>
          <para>An external communications layer.</para>
        </listitem>

        <listitem>
          <para>Client interfaces which provide both end-user services and
          system management tools.</para>
        </listitem>

        <listitem>
          <para>Auxiliary components to support monitoring and to facilitate
          loading and storing of file system data from external sources</para>
        </listitem>
      </itemizedlist>

      <para><graphic fileref="images/ML008.jpg" /></para>

      <para>For more information about the HPCC architecture, clusters and
      components, see the HPCC website, <ulink
      url="http://hpccsystems.com/Why-HPCC/How-it-works">http://hpccsystems.com/Why-HPCC/How-it-works</ulink>.</para>
    </sect1>

    <sect1 id="ECL_Programming_Language">
      <title>The ECL Programming Language</title>

      <para>The ECL programming language is a key factor in the flexibility
      and capabilities of the HPCC processing environment. It is designed to
      be a transparent and implicitly parallel programming language for
      data-intensive applications. It is a high-level, highly-optimized,
      data-centric declarative language that allows programmers to define what
      the data processing result should be and the dataflows and
      transformations that are necessary to achieve the result.</para>

      <para>Execution is not determined by the order of the language
      statements, but from the sequence of dataflows and transformations
      represented by the language statements. It combines data representation
      with algorithm implementation, and is the fusion of both a query
      language and a parallel data processing language.</para>

      <para>ECL uses an intuitive syntax which has taken cues from other
      familiar languages, supports modular code organization with a high
      degree of reusability and extensibility, and supports high-productivity
      for programmers in terms of the amount of code required for typical
      applications compared to traditional languages like Java and C++. It is
      compiled into optimized C++ code for execution on the HPCC system
      platform, and can be used for complex data processing and analysis jobs
      on a Thor cluster or for comprehensive query and report processing on a
      Roxie cluster.</para>

      <para>ECL allows inline C++ functions to be incorporated into ECL
      programs, and external programs in other languages can be incorporated
      and parallelized through a PIPE facility.</para>

      <para>External services written in C++ and other languages which
      generate DLLs can also be incorporated in the ECL system library, and
      ECL programs can access external Web services through a standard
      SOAPCALL interface.</para>

      <para>The ECL language includes extensive capabilities for data
      definition, filtering, data management, and data transformation, and
      provides an extensive set of built-in functions to operate on records in
      datasets which can include user-defined transformation functions. The
      Thor system allows data transformation operations to be performed either
      locally on each node independently in the cluster, or globally across
      all the nodes in a cluster, which can be user-specified in the ECL
      language.</para>

      <para>An additional important capability provided in the ECL programming
      language is support for natural language processing (NLP) with PATTERN
      statements and the built-in PARSE operation. Using this capability of
      the ECL language it is possible to implement parallel processing from
      information extraction applications across document files including
      XML-based documents or Web pages.</para>

      <para>Some benefits of using ECL are:</para>

      <itemizedlist>
        <listitem>
          <para>It incorporates transparent and implicit data parallelism
          regardless of the size of the computing cluster and reduces the
          complexity of parallel programming increasing development
          productivity.</para>
        </listitem>

        <listitem>
          <para>It enables the implementation of data-intensive applications
          with huge volumes of data previously thought to be intractable or
          infeasible. ECL was specifically designed for manipulation of data
          and query processing. Orders of magnitude performance increases over
          other approaches are possible.</para>
        </listitem>

        <listitem>
          <para>The ECL compiler generates highly optimized C++ for
          execution.</para>
        </listitem>

        <listitem>
          <para>It is a powerful, high-level, parallel programming language
          ideal for implementation of ETL, information retrieval, information
          extraction, record linking and entity resolution, and many other
          data-intensive applications.</para>
        </listitem>

        <listitem>
          <para>It is a mature and proven language but is still evolving as
          new advancements in parallel processing and data-intensive computing
          occur.</para>
        </listitem>
      </itemizedlist>

      <para>The HPCC platform also provides a comprehensive IDE (ECL IDE)
      which provide a highly-interactive environment for rapid development and
      implementation of ECL applications.</para>

      <para>HPCC and the ECL IDE downloads are available from the HPCC systems
      website, <ulink
      url="http://hpccsystems.com/">http://hpccsystems.com/</ulink> which also
      provides access to documentation and tutorials.</para>
    </sect1>

    <sect1 id="ECL_IDE">
      <title>ECL IDE</title>

      <para>ECL IDE is an ECL programmer's tool. Its main use is to create
      queries and ECL files and is designed to make ECL coding as easy as
      possible. It has all the ECL built-in functions available to you for
      simple point-and-click use in your query construction. For example, the
      Standard String Library (Std.Str) contains common functions to operate
      on STRING fields such as the ToUpperCase function which converts
      characters in a string to uppercase.</para>

      <para>You can mix-and-match your data with any of the ECL built-in
      functions and/or ECL files you have defined to create Queries. Because
      ECL files build upon each other, the resulting queries can be as complex
      as needed to obtain the result.</para>

      <para>Once the Query is built, submit it to an HPCC cluster, which will
      process the query and return the results.</para>

      <para>Configuration files (.CFG) are used to store the information for
      any HPCC you want to connect to, for example, it stores the location of
      the HPCC and the location of any folders containing ECL files that you
      may want to use while developing queries. These folders and files are
      shown in the <emphasis role="bold">Repository</emphasis> window.</para>

      <para><graphic fileref="images/CT06.jpg" /></para>

      <para>For more information on using ECL IDE see the Client Tools manual
      which may be downloaded from the HPCC website, <ulink
      url="http://hpccsystems.com/community/docs/client-tools">http://hpccsystems.com/community/docs/client-tools</ulink>.</para>
    </sect1>

    <sect1 id="Installing_Using_ML">
      <title>Installing and using the ML Libraries</title>

      <para>The ML Libraries can only be used in conjunction with an HPCC
      System, ECL IDE and the Client tools.</para>

      <sect2>
        <title>Requirements</title>

        <para>If you don't already use the HPCC platform and/or ECL IDE and
        the Client Tools, you must download and install them before
        downloading the ML libraries:</para>

        <itemizedlist>
          <listitem>
            <para>Download and install the relevant HPCC platform for your
            needs. (<ulink
            url="http://hpccsystems.com/download/free-community-edition">http://hpccsystems.com/download/free-community-edition</ulink>)</para>
          </listitem>

          <listitem>
            <para>Download and install the ECL IDE and Client Tools (<ulink
            url="http://hpccsystems.com/download/free-community-edition/ecl-ide-and-client-tools">http://hpccsystems.com/download/free-community-edition/ecl-ide-and-client-tools</ulink>)</para>
          </listitem>
        </itemizedlist>

        <para>The HPCC Systems website provides more information, about the
        HPCC and ECL IDE.</para>

        <para>If you are new to the ECL Language, take a look at the a
        programmers guide and language reference guides, <ulink
        url="http://hpccsystems.com/community/docs/learning-ecl">http://hpccsystems.com/community/docs/learning-ecl</ulink>.</para>

        <para>The HPCC Systems website also provides tutorials designed to get
        you started using data on the HPCC System, <ulink
        url="http://hpccsystems.com/community/docs/tutorials">http://hpccsystems.com/community/docs/tutorials</ulink>.</para>
      </sect2>

      <sect2>
        <title>Installing the ML Libraries</title>

        <para>To install the ML Libraries:</para>

        <orderedlist>
          <listitem>
            <para>Go to the Machine Learning page of the HPCC Systems website,
            <ulink
            url="http://hpccsystems.com/ml">http://hpccsystems.com/ml</ulink>
            and click on <emphasis role="bold">Download and Get
            Started</emphasis>.</para>
          </listitem>

          <listitem>
            <para>Click on <emphasis role="bold">Step 1: Download the ML
            Library</emphasis> and save the file to your computer.</para>
          </listitem>

          <listitem>
            <para>Extract the downloaded files to your ECL IDE source
            folder.</para>

            <para>This folder is typically located here:
            "C:\Users\Public\Documents\HPCC Systems\ECL\My Files".</para>

            <para>To find out the location of this folder, simply go to your
            ECL IDE <emphasis role="bold">Preferences</emphasis> window either
            from the login dialog or from the <emphasis
            role="bold">Orb</emphasis> menu.</para>

            <para>Click on the <emphasis role="bold">Compiler</emphasis> tab
            and use the first <emphasis role="bold">Working Folder</emphasis>
            location listed.</para>
          </listitem>
        </orderedlist>
      </sect2>

      <sect2>
        <title>Using the ML Libraries</title>

        <para>ML module walk-throughs are provided in this manual, which are
        designed to get you started using ML with the HPCC System.</para>

        <para>Now that the ML Libraries are installed, you can use ECL IDE to
        write queries to analyze your data:</para>

        <orderedlist>
          <listitem>
            <para>Login to ECL IDE, accessing the HPCC System you have
            installed.</para>
          </listitem>

          <listitem>
            <para>Using the <emphasis role="bold">Repository</emphasis>
            toolbox, expand <emphasis role="bold">My Files</emphasis>.</para>

            <para>Notice that the <emphasis role="bold">ML</emphasis> folder
            and its contents are now available for you to use in your
            code.</para>
          </listitem>

          <listitem>
            <para>Open a new builder window and start writing your
            query.</para>

            <para>To reference the ML libraries in your ECL source code, use
            an import statement.</para>

            <para>For example:<programlisting><?dbfo keep-together="always"?>IMPORT * FROM ML;
 
IMPORT * FROM ML.Cluster;
 
IMPORT * FROM ML.Types;
 
x2 := DATASET([
{1, 1, 1}, {1, 2, 5},
{2, 1, 5}, {2, 2, 7},
{3, 1, 8}, {3, 2, 1},
{4, 1, 0}, {4, 2, 0},
{5, 1, 9}, {5, 2, 3},
{6, 1, 1}, {6, 2, 4},
{7, 1, 9}, {7, 2, 4}],NumericField);
 
c := DATASET([
{1, 1, 1}, {1, 2, 5},
{2, 1, 5}, {2, 2, 7},
{3, 1, 9}, {3, 2, 4}],NumericField);
 
x3 := Kmeans(x2,c);
 
OUTPUT(x3);</programlisting></para>
          </listitem>
        </orderedlist>

        <para></para>
      </sect2>
    </sect1>

    <sect1 id="Contributing_Sources">
      <title>Contributing to the sources</title>

      <para>Both HPCC and ECL-ML are open source projects and contributions to
      the sources are welcome. If you are interested in contributing to these
      projects, simply download the GitHub client and go to the relevant
      GitHub pages.</para>

      <itemizedlist>
        <listitem>
          <para>To contribute to the HPCC open source project, go to <ulink
          url="https://github.com/hpcc-systems/HPCC-Platform">https://github.com/hpcc-systems/HPCC-Platform</ulink>.</para>
        </listitem>

        <listitem>
          <para>To contribute to the ECL-ML open source project, go to <ulink
          url="https://github.com/hpcc-systems/ecl-ml">https://github.com/hpcc-systems/ecl-ml</ulink>.</para>
        </listitem>
      </itemizedlist>

      <para>You are required to sign a contribution agreement to become a
      contributor.</para>
    </sect1>
  </chapter>

  <chapter id="Machine_Learning_Algorithms">
    <title>Machine Learning Algorithms</title>

    <para>The HPCC Systems Machine Learning libraries contain an extensible
    collection of machine learning routines which are easy and efficient to
    use and are designed to execute in parallel across a cluster. T</para>

    <para>he list of modules supported will continue to grow over time. The
    following modules are currently supported:</para>

    <itemizedlist>
      <listitem>
        <para>Associations (ML.Associate)</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para>Classify (ML.Classify)</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para>Cluster (ML.Cluster)</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para>Correlations (ML.Correlate)</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para>Discretize (ML.Discretize)</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para>Distribution (ML.Distribution)</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para>Field Aggregates (ML.FieldAggregates)</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para>Regression (ML.Regression)</para>
      </listitem>
    </itemizedlist>

    <para>The Machine Learning modules are supported by the following which
    are also used to implement ML:</para>

    <itemizedlist>
      <listitem>
        <para>The Matrix Library (Mat)</para>
      </listitem>

      <listitem>
        <para>Utility (ML.Utility)</para>
      </listitem>

      <listitem>
        <para>Docs (ML.Doc)</para>
      </listitem>
    </itemizedlist>

    <para>The ML Modules are used in conjunction with the HPCC system. More
    information about the HPCC System is available on the following website,
    <ulink url="http://hpccsystems.com/"><ulink
    url="http://hpccsystems.com/">http://hpccsystems.com/</ulink></ulink>.</para>

    <sect1 id="ML_Data_Models">
      <title>The ML Data Models</title>

      <para>The ML routines are all centered around a small number of core
      processing models. As a user of ML (rather than an implementer) the
      exact details of these models can generally be ignored. However, it is
      useful to have some idea of what is going on and what routines are
      available to help you with the various models. The formats that are
      shared between various modules within ML are all contained within the
      Type definition.</para>

      <sect2 id="Numeric_Field">
        <title>Numeric field</title>

        <para>By default, the ToField operation assumes the first field is the
        âidâ field, and all subsequent numeric fields are to be assigned a
        field number in the resulting table. However, additional parameters
        may be specified to ToField that facilitates the ability to specify
        the name of the id column in the original table as well as the columns
        to be used as data fields.</para>

        <para>For example:</para>

        <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;

value_record := RECORD
STRING first_name;
STRING last_name;
UNSIGNED name_id;
REAL height;
REAL weight;
REAL age;
STRING eye_color;
INTEGER1 species; // 1 = human, 2 = tortoise
INTEGER1 gender; // 0 = unknown, 1 = male, 2 = female
END;
dOrig := dataset([
{'Charles','Babbage',1,5*12+7,156*16,43,'Blue',1,1},
{'Tim','Berners-Lee',2,5*12+7,128*16,31, 'Brown',1,1},
{'George','Boole',3,5*12+9,135*16,15, 'Hazel',1,1},
{'Herman','Hollerith',4,5*12+7,145*16,14,'Green',1,1},
{'John','Von Neumann',5,5*12-2,80*16,9,'Blue',1,1},
{'Dennis','Ritchie',6,4*12+8,72*16,8, 'Brown',1,1},
{'Alan','Turing',7,8,32,2.5, 'Brown',2,1}
],value_record);
ML.ToField(dOrig,dResult,name_id,'height,weight,age,gender');
dOrig;
dResult;

</programlisting>In the above example, the name_id column is taken as the id.
        Height, weight, age and gender will be parsed into numbered fields.
        Note that the id name is not in quotes, but the comma-delimited list
        of fields is.</para>

        <para>Along with creating the new table in NumericField format, the
        ToField macro also creates three other objects to help with field
        translation, two functions and a dataset.</para>

        <para>The two functions are outtable_ToName() and outtable_ToNumber(),
        where outtable is the name of the output table specified in the macro
        call. Passing in a number in the first one will produce the field name
        mapped to that number, and passing a string into the second one will
        produce the number assigned to that field name.</para>

        <para>For our above example, we can therefore do the following:</para>

        <para><programlisting><?dbfo keep-together="always"?>dResult_ToName(2);      // Returns âweightâ
dResult_ToNumber(âageâ) // Returns 3 (note that the field name is always lowercase)
</programlisting></para>

        <para>The other dataset that is created is a 2-column mapping table
        named outtable_Map which contains every field from the original table
        in the first column, and what it is mapped to in the second
        column.</para>

        <para>This would either be the column number, the string âIDâ if it is
        the ID field, or the string âNAâ indicating that the field was not
        mapped to a NumericField number. In the above example, the table is
        named:</para>

        <para><programlisting><?dbfo keep-together="always"?>dResult_Map;
</programlisting></para>

        <para>The mapping table may be used when reconstituting the data back
        to the original format. For example:</para>

        <para><programlisting><?dbfo keep-together="always"?>ML.FromField(dResult,value_record,dReconstituted,dResult_Map);
dReconstituted;
</programlisting></para>

        <para>The output from this FromField call will have the same structure
        as the initial table, and values that existed in the NumericField
        version of the table will be allocated to the fields specified in the
        mapping table.</para>

        <para>An important note is that any data that did not translate into
        the NumericField table will be left blank or zero in the reconstituted
        table.</para>
      </sect2>

      <sect2 id="Discrete_field">
        <title>Discrete field</title>

        <para>Some of the ML routines do not require the field values to be
        real, rather they require discrete (integral) values. The structure of
        the records are essentially identical to NumericField but, the value
        is of type t_Discrete (typically INTEGER) rather than t_FieldReal
        (typically REAL8).</para>

        <para>There are no explicit routines to get to a discrete-field
        structure from an ECL record, rather it is presumed that NumericField
        will be used as an intermediary.</para>

        <para>There is an entire module (Discretize) devoted to moving a
        NumericField structured file into a DiscreteField structured file. The
        options and reasons for the options are described in the Discretize
        module section. For this introduction it is adequate to show that all
        of the numeric fields could be made integral simply by using:</para>

        <para><programlisting><?dbfo keep-together="always"?>ml.ToField(d,o);
o;
o1 := ML.Discretize.ByRounding(o);
o1</programlisting></para>
      </sect2>

      <sect2 id="ItemElement">
        <title>ItemElement</title>

        <para>A rather more specialist format is the ItemElement format. This
        does not model an ECL record directly, rather it models an abstraction
        that can be derived from an ECL record.</para>

        <para>The item element has a record id and a value (which is of type
        t_Item). The t_Item is an integral value â but unlike t_Discrete the
        values are not considered to be ordinal. Put another way, in
        t_Discrete 4 &gt; 3 and 2 &lt; 3. In t_Item the 2, 3, 4 are just
        arbitrary labels that âhappenâ to be integers for efficiency. Note
        also, that ItemElement does not have a field number. There is no
        significance placed upon the field from which the value was derived.
        This models the abstract notion of a collection of âbagsâ of items. An
        example of the use of this type of structure will be given in the
        Using ML with documents, section.</para>
      </sect2>

      <sect2 id="Coding_ML_Data_Models">
        <title>Coding with the ML data models</title>

        <para>The ML data models are extremely flexible to work with; but
        using them is a little different from traditional ECL programming.
        This section aims to detail some of the possibilities.</para>

        <sect3 id="Column_Splitting">
          <title>Column splitting</title>

          <para>Some of the ML routines expect to be handed two datasets which
          may be, for example, a dataset of independent variables and another
          of dependent variables. The data as it originally exists will
          usually have the independent and dependent data within the same row.
          For example, when using a classifier to produce a model to predict
          the species or gender of an entity from the other details, the
          height, weight and age fields would need to be in a different âfileâ
          to the species and gender. However, they have to have the same
          record ID to show the correlation between the two. In the ML data
          model this is as simple as applying two filters: <programlisting><?dbfo keep-together="always"?>ml.ToField(d,o);
o1 := ML.Discretize.ByBucketing(o,5);
Independents := o1(Number &lt;= 3);
Dependents := o1(Number &gt;= 4);
Bayes := ML.Classify.BuildNaiveBayes(Independents,Dependents);
Bayes
</programlisting></para>
        </sect3>

        <sect3 id="Genuine_Nulls">
          <title>Genuine nulls</title>

          <para>Implementing a genuine null can be done by simply removing
          certain fields with certain values from the datastream. For example,
          if 0 was considered an invalid weight then one could
          do:<programlisting><?dbfo keep-together="always"?>Better := o(Number&lt;&gt;2 OR Value&lt;&gt;0);</programlisting></para>
        </sect3>

        <sect3 id="Sampling">
          <title>Sampling</title>

          <para>By far the easiest way to split a single data file into
          samples is to use the SAMPLE and ENTH verbs upon the datafile PRIOR
          to the conversion to ML format.</para>
        </sect3>

        <sect3 id="Inserting_Column_Computed_Value">
          <title>Inserting a column with a computed value</title>

          <para>Inserting a column with a new value computed from another
          field value is a fairly advanced technique. The following inserts
          the square of the weight as a new column:<programlisting><?dbfo keep-together="always"?>ml.ToField(d,o);

BelowW := o(Number &lt;= 2); 
// Those columns whose numbers are not changed
// Shuffle the other columns up - this is not needed if appending a column
AboveW := PROJECT(o(Number&gt;2),TRANSFORM(ML.Types.NumericField,SELF.Number := 
LEFT.Number+1, SELF := LEFT));
NewCol := PROJECT(o(Number=2),TRANSFORM(ML.Types.NumericField,
                                                        SELF.Number := 3,
                                                        SELF.Value := LEFT.Value*LEFT.Value,
                                                        SELF := LEFT) );
    
NewO := BelowW+AboveW+NewCol;

NewO;
</programlisting></para>
        </sect3>
      </sect2>
    </sect1>

    <sect1 id="Generating_test_data">
      <title>Generating test data</title>

      <para>ML is interesting when it is being executed against data with
      meaning and significance. However, sometimes it can be useful to get
      hold of a lot of data quickly for testing purposes. This data may be
      ârandomâ (by some definition) or it may follow a number of carefully
      planned statistical distributions. The ML libraries have support for
      high performance ârandom valueâ generation using the GenData command
      inside the distribution module.</para>

      <para>GenData generates one column at a time although it generates that
      column for all the records in the file. It works in parallel so is very
      efficient.</para>

      <para>The easiest type of column to generate is one in which the values
      are evenly and randomly distributed over a range. The following
      generates 1M records each with a random number from 0-100 in the first
      column:</para>

      <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;

TestSize := 1000000;
a1 := ML.Distribution.Uniform(0,100,10000); 
ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform

</programlisting></para>

      <para>To generate 1M records with three columns; one Uniformly
      distributed, one Normally distributed (mean 0, Standard Deviation 10)
      and one with a Poisson distribution (Mean of 4):</para>

      <programlisting><?dbfo keep-together="always"?>IMPORT ML;

TestSize := 1000000;

a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
// Field 2 Normally Distributed
a2 := ML.Distribution.Normal2(0,10,10000);
b2 := ML.Distribution.GenData(TestSize,a2,2);
// Field 3 - Poisson Distribution
a3 := ML.Distribution.Poisson(4,100);
b3 := ML.Distribution.GenData(TestSize,a3,3);

D := b1+b2+b3; // This is the test data

ML.FieldAggregates(D).Simple;  // Perform some statistics on the test data to ensure 
                                  it worked
</programlisting>

      <para>This generates the data in the correct format and even produces
      some statistics to ensure it works!</para>

      <para>The ML libraries have over half a dozen different distributions
      that the generated data columns can be given. These are described at
      length in the Distribution module section.</para>
    </sect1>
  </chapter>

  <chapter id="ML_module_walkthroughs">
    <title>ML module walk-throughs</title>

    <para>To help you get started, a walk-through is provided for each ML
    module. The walk-throughs explain how the modules work and demonstrate how
    they can be used to generate the results you require.</para>

    <sect1 id="Classify_walkthrough">
      <title>Classification walk-through</title>

      <para><emphasis role="bold">Modules: Classify, Regression,
      Doc</emphasis></para>

      <para>Classification techniques tackle the problem of identifying which
      groups new observations belong to, based on learned characteristics of
      an equivalent training set. Based upon a set of observations and groups
      provided in a training set, can we correctly predict the group of an
      observation outside of the training set. This is really where data
      processing gives way to machine learning. Based upon some form of
      training set can I derive a rule or model to predict something about
      other data records?</para>

      <para>Classification is sufficiently central to machine learning that we
      provide three different methods of doing it. You will need to examine
      the literature or experiment to decide exactly which method of
      classification will work best in any given context. In the examples that
      follow we are simply trying to show how a given method can be used in a
      given context and we are not necessarily claiming it is the best or only
      way to solve the given example.</para>

      <sect2 id="Naive_Bayes">
        <title>Naive Bayes</title>

        <para>The concept behind NaÃ¯ve Bayes is that every property of an
        object is in and of itself a predictor of the type of that object. By
        summing all of the predictions for all of the properties of an object,
        you can compute the type the object is most likely to have. Put
        another way, Bayes assumes that all of the properties of an object are
        independent hence the expression âNaÃ¯veâ.</para>

        <para>Another interesting feature (and benefit) of Bayes is that while
        it predicts ordinal values (integers â 0, 1, 2 etc), it is not
        producing a âscoreâ so, it does not restrict the integers to being âin
        sequenceâ. So a property of an object might predict a 0 or 2, but it
        will not predict a 1. Bayes will not predict anything if handed
        totally random data. It is precisely looking for a relationship
        between the data that is non-random. The following example generates
        test data by:</para>

        <orderedlist>
          <listitem>
            <para>Generating three random columns.</para>
          </listitem>

          <listitem>
            <para>Producing a fourth column that is the sum of the three
            columns.</para>
          </listitem>

          <listitem>
            <para>Giving the fourth column a category from 0 (small) to 2
            (big).</para>
          </listitem>
        </orderedlist>

        <para>The purpose is to see if the system can learn to predict which
        category the record will be assigned using the individual fields. The
        data generation is a little more complex than normal so it is
        presented here. In the rest of this section if a âD1â appears from
        nowhere, it is referencing this dataset.</para>

        <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
TestSize := 10000000;
a1 := ML.Distribution.Poisson(5,100); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
a3 := ML.Distribution.Poisson(3,100);
b3 := ML.Distribution.GenData(TestSize,a3,3);

D := b1+b2+b3; // This is the test data
// Now construct a fourth column which is the sum of them all
B4 := PROJECT(TABLE(D,{Id,Val := SUM(GROUP,Value)},Id),TRANSFORM(ML.Types.NumericField,
                                           SELF.Number:=4,
                                           SELF.Value:=MAP(LEFT.Val &lt; 6 =&gt; 0,  // Small
                                                                    &lt; 10 =&gt; 1, // Normal
                                                                          2 ); // Big
                                           SELF := LEFT));
D1 := D+B4;
</programlisting>When generating and preparing data for NaÃ¯ve Bayes you need
        to be aware that Bayes is looking for discrete numbers as input (not
        the real numbers generated). The Discretize module can do this for
        you. This module is described in detail in the chapter The ML
        Modules.</para>

        <para>To keep things simple we will just round all the fields to
        integers in this example:</para>

        <para><programlisting><?dbfo keep-together="always"?>D2 := ML.Discretize.ByRounding(D1);</programlisting>It
        is now possible to âtrainâ the NaÃ¯ve Bayes classifier using the
        BuildNaiveBayes definition:<programlisting><?dbfo keep-together="always"?>ML.Classify.BuildNaiveBayes(D2(Number&lt;=3),D2(Number=4))</programlisting></para>

        <para>Notice that all of the values I âknowâ are being passed in as
        the first parameter (these are the independent variables). All of the
        ones I want to learn (the dependents) are being passed in as the
        second parameter. In this case there is only one column of dependent
        variables but the BuildNaiveBayes is capable of constructing multiple
        Bayesian models at once.</para>

        <para>One thing that seems to shock some people is that a NaiveBayes
        classifier is not perfect. Even if you construct a Bayes model, you
        can hand the classifier back the training data and it may still
        mis-classify some of the data. This is not a bug. It is a restriction
        of the mathematical model that underlies Bayes. This obviously begs
        the question, "well, it may not be perfect but just how good is
        it?".</para>

        <para>The Classify module has a definition, TestNaiveBayes, which
        takes in a set of independent variables, a set of actual outcomes
        (dependent variables) and a Bayes model. It will then return a slew of
        results to show how good (or bad) the classifier was. The following
        results are produced:</para>

        <informaltable>
          <tgroup cols="2">
            <colspec align="left" colwidth="110pt" />

            <colspec align="left" colwidth="480pt" />

            <thead>
              <row>
                <entry align="left">Result</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry align="left">Headline</entry>

                <entry>Gives you the main precision number. On this test data,
                the result shows how often was the classifier correct.</entry>
              </row>

              <row>
                <entry align="left">PrecisionByClass</entry>

                <entry>Similar to Headline except that it gives the precision
                broken down by the class that it SHOULD have been classified
                to. It is possible that a classifier might work well but may
                be particularly poor at identifying one of the groups.</entry>
              </row>

              <row>
                <entry align="left">CrossAssignments</entry>

                <entry>It is one thing to say a classification is âwrongâ.
                This table shows, âif a particular class is mis-classified,
                what is it most likely to be mis-classified as?â.</entry>
              </row>

              <row>
                <entry align="left">Raw</entry>

                <entry>Gives a very detailed breakdown of every record in the
                test corpus, for example, what the classification should have
                been and what it was.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable>

        <para>Assuming you are happy with your model the final step is to use
        it!</para>

        <para>This is very easy to do because the Classify module has a
        NaiveBayes definition which takes the independent data and the model
        you supply and produces the dependent columns you require. In the
        following example, this is done using the training set we used
        earlier.50</para>

        <para>For real work this would be a completely different dataset that
        was being processed:</para>

        <para><programlisting><?dbfo keep-together="always"?>Model := ML.Classify.BuildNaiveBayes(D2(Number&lt;=3),D2(Number=4));
Results := ML.Classify.NaiveBayes(D2(Number&lt;=3),Model);
Results 
</programlisting></para>

        <para></para>
      </sect2>

      <sect2 id="Logisitic_regression">
        <title>Logistic Regression</title>

        <para>Regression analysis includes techniques for modeling the
        relationship between a dependent variable Y and one or more
        independent variables Xi.</para>

        <para>The ML.Regression module currently provides two models, OLS and
        Poly, where the relationship is expressed as linear and polynomial
        function respectively.</para>

        <para>When dependent variable Y is binary, ie when Y takes either 0 or
        1 values, neither linear nor polynomial function can model the
        relationship between X and Y correctly. In that case, the relationship
        can be modeled using a logistic function, also known as sigmoid
        function, which is an S-shaped curve with values from (0,1).</para>

        <para>Since dependent variable Y can take only two values, 0 or 1, the
        Logistic Regression model predicts two outcomes 0 or 1, and it can be
        used as a tool for classification.</para>

        <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;

value_record:= RECORD
    unsigned   rid;
    real       age;
    real       height;
    integer1   sex; // 0 = female, 1 = male
END;

d := DATASET([{1,35,149,0},{2,11,138,0},{3,12,148,1},{4,16,156,0},
            {5,32,152,0},{6,16,157,0},{7,14,165,0},{8,8,152,1},
            {9,35,177,0},{10,33,158,1},{11,40,166,0},{12,28,165,0},
            {13,23,160,0},{14,52,178,1},{15,46,169,0},{16,29,173,1},
            {17,30,172,0},{18,21,163,0},{19,21,164,0},{20,20,189,1},
            {21,34,182,1},{22,43,184,1},{23,35,174,1},{24,39,177,1},
            {25,43,183,1},{26,37,175,1},{27,32,173,1},{28,24,173,1},
            {29,20,162,0},{30,25,180,1},{31,22,173,1},{32,25,171,1}]
            ,value_record);

ML.ToField(d,flds);
logistic := ML.Classify.Logistic(flds(Number=2),flds(Number=3));
logistic.Beta;
logistic.modelY;
</programlisting>In this example we have a dataset containing age, height in
        centimeters and sex of 32 people, and we would like to create a
        logistic model that will make it possible to predict a person's sex
        given a person's height. The logistic model is built using the height
        column as the independent variable, and using the sex column as the
        dependent variable. The logistic.Beta calculates parameters of the
        model using the iteratively-reweighted least squares (IRLS) algorithm:
        Î²= (Î²0, Î²1), and those parameters can be plugged into the model
        function Y = 1 ./ (1+exp(-X*Î²)) to predict the sex of a person X with
        the specified height. If the model-calculated value Y is greater than
        0.5, then the sex of the person X is predicted to be male, otherwise
        the sex is predicted to be female. The logistic.modelY returns the
        model-calculated (ie predicted) sex for all the heights provided by
        the independent variable X.</para>
      </sect2>
    </sect1>

    <sect1 id="Cluster_Walkthrough">
      <title>Cluster Walk-through</title>

      <para><emphasis role="bold">Modules: Cluster, Doc</emphasis></para>

      <para>The cluster module contains routines that can be used to find
      groups of records that appear to be âfairly similarâ.</para>

      <para>The module has been shown to work on records with as few as two
      fields and as many as sixty thousand. The latter was used for clustering
      documents of words (see Using ML with documents). The clustering module
      has more than half a dozen different ways of measuring the distance
      (defining âsimilarâ) between two records but it is also possible to
      write your own.</para>

      <para>Below are walk-throughs for the methods covered in the ML.Cluster
      module. Each begins with the following set of entities in 2-dimensional
      space, where the values on each axis are restricted to between 0.0 and
      10.0:</para>

      <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
lMatrix:={UNSIGNED id;REAL x;REAL y;};

dEntityMatrix:=DATASET([
  {1,2.4639,7.8579},
  {2,0.5573,9.4681},
  {3,4.6054,8.4723},
  {4,1.24,7.3835},
  {5,7.8253,4.8205},
  {6,3.0965,3.4085},
  {7,8.8631,1.4446},
  {8,5.8085,9.1887},
  {9,1.3813,0.515},
  {10,2.7123,9.2429},
  {11,6.786,4.9368},
  {12,9.0227,5.8075},
  {13,8.55,0.074},
  {14,1.7074,3.9685},
  {15,5.7943,3.4692},
  {16,8.3931,8.5849},
  {17,4.7333,5.3947},
  {18,1.069,3.2497},
  {19,9.3669,7.7855},
  {20,2.3341,8.5196}
],lMatrix);
ML.ToField(dEntityMatrix,dEntities);
</programlisting></para>

      <para>Note that the use of the ToField macro which converts the original
      rectangular matrix, dEntityMatrix, into a table in the standard
      NumericField format used by the ML library named âdEntitiesâ.</para>

      <sect2 id="kmeans">
        <title>KMeans</title>

        <para>With k-means clustering the user creates a second set of
        entities called centroids, with coordinates in the same space as the
        entities being clustered. The user defines the number of centroids (k)
        to create, which will remain constant during the process and therefore
        represents the number of clusters that will be determined. For our
        example, we will define four centroids:</para>

        <para><programlisting><?dbfo keep-together="always"?>dCentroidMatrix:=DATASET([
  {1,1,1},
  {2,2,2},
  {3,3,3},
  {4,4,4}
],lMatrix);

ML.ToField(dCentroidMatrix,dCentroids);
</programlisting>As with the entity matrix, we have used ToField to convert
        the centroid matrix into the table âdCentroidsâ.</para>

        <para>Note that although these points are arbitrary, they are clearly
        not random. These points form an asymmetrical pattern in one corner of
        the space. This is to highlight a feature of k-means clustering which
        is that the centroids will end up in the same resting place (or very
        close to it) regardless of where they started. The only caveat related
        to centroid positioning is that no two centroids should occupy the
        same initial location.</para>

        <para>Now that we have our centroids, they are now subjected to a
        2-step iterative re-location process. For each iteration we determine
        which entities are closest to which centroids, then we recalculate the
        position of the centroids based as the mean location of all of the
        entities affiliated with them.</para>

        <para>To set up this process, we make the following call to the KMeans
        routine:</para>

        <para><programlisting><?dbfo keep-together="always"?>MyKMeans:=ML.Cluster.KMeans(dEntities,dCentroids,30,.3);
</programlisting>Here, we are passing in our two datasets, dEntities and
        dCentroids. In order to prevent infinite loops, we also must specify a
        maximum number of iterations, which is set to 30 in the above
        example.</para>

        <para>Convergence is defined as the point at which we can say the
        centroids have found their final resting places. Ideally, this will be
        when they stop moving completely.</para>

        <para>However, there will be situations where centroids may experience
        a âsee-sawingâ action, constantly trading affiliations back and forth
        indefinitely. To address this, we have the option of specifying a
        positive value as the convergence threshold. The process will assume
        convergence if, during any iteration, no centroid moves a distance
        greater than that number. In our above example, we are setting the
        convergence threshold to 0.3. If no threshold is specified, then the
        threshold is set to 0.0. If the process hits the maximum number of
        iterations passed in as parameter 3, then it stops regardless of
        whether convergence is achieved or not.</para>

        <para>The final parameter, which is also optional, specifies which
        distance formula to use. For our example we are leaving this parameter
        blank, so it defaults to a simple Euclidean calculation, but we could
        easily change this by adding the fifth parameter with a value such as
        âML.Cluster.DF.Tanimotoâ or âML.Cluster.DF.Manhattanâ.</para>

        <para>Below are calls to the available attributes within the KMeans
        module:</para>

        <para><programlisting><?dbfo keep-together="always"?>MyKMeans.AllResults;
</programlisting>This will produce a table with a layout similar to
        NumericField, but instead of a single value field, we have a field
        named âvaluesâ which is a set of values.</para>

        <para>Each row will have the same number of values in this set, which
        is equal to the number of iterations + 1. Values[1] is the initial
        value for the id/number combination, Values[2] is after the first
        iteration, etc.</para>

        <para><programlisting><?dbfo keep-together="always"?>MyKMeans.Convergence;</programlisting>Convergence
        will respond with the number of iterations that were performed, which
        will be an integer between 1 and the maximum specified in the
        parameters. If it is equal to the maximum, then you may want to
        increase that number or specify a higher convergence threshold because
        it had not yet achieved convergence when it completed.</para>

        <para><programlisting><?dbfo keep-together="always"?>MyKMeans.Result();  // The final locations of the centroids
MyKMeans.Result(3); // The results of iteration 3
</programlisting>Results will respond with the centroid locations after the
        specified number of iterations.</para>

        <para>If no number is passed, this will be the locations after the
        final iteration.</para>

        <para><programlisting><?dbfo keep-together="always"?>MyKMeans.Delta(3,5); // The distance every centroid travelled across each axis from 
                        iterations 3 to 5
MyKMeans.Delta(0);   // The total distance the centroids travelled on each axis from the 
                        beginning to the end
</programlisting>Delta displays the distance traveled <emphasis>on each
        axis</emphasis> between the iterations specified in the parameters. If
        no parameters are passed, this will be the delta between the last two
        iterations.</para>

        <para><programlisting><?dbfo keep-together="always"?>MyKMeans.DistanceDelta(3,5); // The straight-line distance travelled by each centroid from 
                                iterations 3 to 5
MyKMeans.DistanceDelta(0);   // The total straight-line distance each centroid travelled 
MyKMeans.DistanceDelta();    // The distance traveled by each centroid during the last 
                                iteration.
</programlisting>DistanceDelta is the same as Delta, but displays the DISTANCE
        delta as calculated using whichever method the KMeans routine was
        instructed to use, which in our example is Euclidean.</para>
      </sect2>

      <sect2 id="AggloN">
        <title>AggloN</title>

        <para>With Agglomerative, or Hierarchical, clustering there is no need
        for a centroid set. This method takes a bottom-up approach whereby it
        identifies those pairs that are mutually closest and marries them so
        they are treated as a single entity during the next iteration. Allowed
        to run until full convergence, every entity will eventually be
        stitched up into a single tree structure with each fork representing
        tighter and tighter clusters.</para>

        <para>We set up this clustering routine using the following
        call:</para>

        <para><programlisting><?dbfo keep-together="always"?>MyAggloN:=ML.Cluster.AggloN(dEntities,4);</programlisting>Here,
        we are passing in our sample data set and telling the routine that we
        want a maximum of 4 iterations.</para>

        <para>There are two further parameters that the user may pass, both of
        which are optional. Parameter 3 enables the user to specify the
        distance formula exactly as we could in Parameter 5 of the KMeans
        routine. And as with our KMeans example, we will leave this blank so
        it defaults to Euclidean.</para>

        <para>Parameter 4 enables us to specify how we want to represent
        distances where clustered entities are involved. After the first
        iteration some of the entities will have been grouped together and we
        need to make a decision about how we measure distance to those groups.
        The three options are min_dist, max_dist, and ave_dist, which will
        instruct the routine to use the minimum distance within the cluster,
        the maximum or the average respectively. The default, which we are
        accepting for this example, is min_dist.</para>

        <para>The following three calls will give us the results of the
        Agglomerative clustering call in different ways:</para>

        <para><programlisting>MyAggloN.Dendrogram;</programlisting>The
        Dendrogram call displays the output as a string representation of the
        tree diagram. Clusters are grouped within curly braces ({}), and
        clusters of clusters are grouped in the same manner. The ID for each
        cluster will be assigned the lowest ID of the entities it encompasses.
        In our example, we end up with five clusters, and two entities yet to
        be clustered. This is because we specified a maximum of four
        iterations which was not enough to group everything together.</para>

        <para><programlisting>MyAggloN.Distances;</programlisting>The
        Distances output displays all of the remaining distances that would be
        used to further cluster the entities. If we had achieved convergence,
        this would be an empty table and our Dendrogram output would be a
        single line with every item found within the tree string. But since we
        stopped iterating early, we still have items to cluster, and therefore
        still have distances to display. The number of rows here will be equal
        to n*n-1, where n is the number of rows in the Dendrogram
        table.</para>

        <para><programlisting>MyAggloN.Clusters;</programlisting>Clusters will
        display each entity, and the ID of the cluster that the entity was
        assigned to. In our example, every entity will be assigned to one of
        the seven cluster IDs found in the Dendrogram. If we had allowed the
        process to continue to convergence, which for our sample set is
        achieved after 9 iterations, every entity will be assigned the same
        cluster ID because it will be the only one left in the
        Dendrogram.</para>
      </sect2>
    </sect1>

    <sect1>
      <title>Correlation Walk-through</title>

      <para>Most of the algorithms within the ML libraries assume that one of
      your inputs are a collection features of a particular object and the
      algorithm exists to predict some other feature based upon the features
      you have. In the literature the âfeatures you haveâ are usually referred
      to as the âindependent variablesâ and the features you are trying to
      predict are called the âdependent variablesâ.</para>

      <para>Masked within those names is an assumption that is almost never
      true. That the features you have for a given object are actually
      independent of each other. Consider, for example, a classification
      algorithm that tries to predict risk of heart disease based upon height,
      weight, age and gender. The independent variables are not even close to
      being independent. Pick any two of those variables and there is a known
      link between them (even age and gender; women live longer). These
      linkages between the âindependentâ variables usually represent an error
      factor in the algorithm used to compute the dependent variable.</para>

      <para>The Correlation module exists to allow you to quantify the degree
      of relatedness between a set of variables. There are three measures
      provided. Under the title âsimpleâ the Covariance and Pearson statistic
      is provided for every pair of variables. The Kendall measure provides
      the Kendal Tau statistic for every pair of variables; it should be noted
      that computation of Kendallâs Tau is an O(N^2) process. This will hurt
      on very large datasets.</para>

      <para>The definition and interpretation of these terms can be found in
      any statistical text; for example: <ulink
      url="http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient</ulink>
      <ulink
      url="http://en.wikipedia.org/wiki/Kendall_tau_rank_correlation_coefficient">http://en.wikipedia.org/wiki/Kendall_tau_rank_correlation_coefficient</ulink></para>

      <para>(we implement tau-a)</para>

      <programlisting>import ml;

value_record := RECORD
                unsigned rid;
  real height;
                real weight;
                real age;
                integer1 species;
                integer1 gender; // 0 = unknown, 1 = male, 2 = female
  END;

d := dataset([{1,5*12+7,156*16,43,1,1},
              {2,5*12+7,128*16,31,1,2},
              {3,5*12+9,135*16,15,1,1},
              {4,5*12+7,145*16,14,1,1},
              {5,5*12-2,80*16,9,1,1},
              {6,4*12+8,72*16,8,1,1},
              {7,8,32,2.5,2,2},
              {8,6.5,28,2,2,2},
              {9,6.5,28,2,2,2},
              {10,6.5,21,2,2,1},
              {11,4,15,1,2,0},
              {12,3,10.5,1,2,0},
              {13,2.5,3,0.8,2,0},
              {14,1,1,0.4,2,0}
             ]
             ,value_record);
// Turn into regular NumericField file (with continuous variables)
ml.ToField(d,o);

Cor := ML.Correlate(o);
Cor.Simple;
Cor.Kendall;
</programlisting>
    </sect1>

    <sect1 id="Discretize_Walkthrough">
      <title>Discretize Walk-through</title>

      <para><emphasis role="bold">Modules: Discretize</emphasis></para>

      <para>As discussed briefly in the section on data models it is not
      unusual for data to be provided in a manner where the data forms some
      real value. For example a height or weight might be measured down to the
      inch or ounce; a price might be measured down to the nearest cent. Yet
      in terms of predictiveness we might expect âsimilarâ values in those
      fields to exhibit similar behavior in some particular regard. Some of
      the ML modules expect the input data to have been âbandedâ; that is for
      data which was originally in âreal valuesâ to have been turn into a set
      of discrete bands. More concretely they require data in the
      DiscreteField format even if it was originally provided in NumericField
      format. The Discretize module exists to perform this conversion. All of
      the examples in this walk-through use the first dataset (d) from the
      NumericField walk-through earlier; it might help you to just quickly
      look at that dataset again in both original and NumericField form to
      remind you of the format.</para>

      <para><programlisting><?dbfo keep-together="always"?>ml.ToField(d,o);
d;
o;
</programlisting>There are currently 3 main methods available to create
      discrete values, ByRounding, ByBucketing and ByTiling.</para>

      <para>All three methods operate upon all the data handed to them.
      Applying different methods to different fields is very easy using the
      methods discussed in the section entitled: âData Modelsâ. This simple
      example Auto-buckets columns 2 &amp; 3 into four bands, tiles column 1
      into 6 bands and the rounds the fourth column to the nearest
      integer:</para>

      <para><programlisting>disc := ML.Discretize.ByBucketing(o(Number IN [2,3]),4)+ML.Discretize.ByTiling(o(Number IN 
                                              [1]),6)+ML.Discretize.ByRounding(o(Number=4));
disc;
</programlisting>It may be observed that in the description above I was able
      to describe how to discretize data a number of different ways but could
      not give any firm guidelines as to the exactly number of bands or the
      exact best method to use for any given item of data. That is because
      whilst there are schemes and guidelines out there, there is no firm
      consensus as to which are the best. It is quite possible that the best
      way to work is to âtry a fewâ and see which gives you the best
      results!</para>

      <para>The Discretize module supports this âsuck it and seeâ approach by
      allowing you to specify the discretization methods entirely within data.
      The core of this is the âDoâ command that takes a series of instructions
      and discretizes a dataset based upon those instructions. Each
      instruction is actually a little record of type r_Method defined in the
      Discretize module and you can construct these records yourself if you
      wish. It would be fairly easy to even create a little âdiscreting
      programming languageâ and have it execute. For the slightly less
      ambitious there are a collection of parameterized functions that will
      construct an r_Method record for each of the three main discretize
      types.</para>

      <para>The following example is exactly equivalent to the
      previous:</para>

      <para><programlisting><?dbfo keep-together="always"?>// Build up the instructions into a single data file (âmini programâ)
inst := ML.Discretize.i_ByBucketing([2,3],4)+ML.Discretize.i_ByTiling([1],6)
                                            +ML.Discretize.i_ByRounding([4]);
// Execute the instructions
done := ML.Discretize.Do(o,inst);
done;

</programlisting></para>

      <sect2 id="ByRounding">
        <title>ByRounding</title>

        <para>The ByRounding method exists to convert a real number to an
        integer by ârounding itâ. At its simplest this means that every real
        number is converted to an integer. Values that were less than a half
        go down to the nearest integer; those that were .5 or above go up.
        Therefore if you have a field that has house prices; perhaps from
        $10,000 to $1M then you potentially will end up with 990,000 different
        discrete values (ever possible dollar value).</para>

        <para>This has made the data discrete but it hasnât really satisfied
        the problem that âsimilar valuesâ have an identical discrete value. We
        might expect a $299,999 dollar house to be quite similar to a $299,998
        dollar house. The ByRounding method therefore has a scale. The real
        number in the data is multiplied by the scale factor PRIOR to
        rounding.</para>

        <para>In our example, if we apply a scale of 0.0001 (1/10000) a
        $299999 house (and a $299998 house) will both get a ByRounding result
        of 30. The scale effectively reduces the range of a variable. In the
        house case a scale of 0.0001 reduces the range from â$10,000 to $1Mâ
        to 1-100 which is much more manageable.</para>

        <para>Sometimes the scaled ranges do not work out so neatly. Suppose
        the field is measuring height of high-school seniors. The original
        range is probably from 48 inches up to possibly 88 inches. A scale of
        0.25 is probably enough to give the number of discrete values you
        require (10), but they will range from 12 to 22 which is not
        convenient. Therefore a DELTA is available, which is ADDED to the
        value AFTER scaling but before rounding. It can therefore be used to
        bring the eventual range down to a convenient number. In this case a
        Delta of -11 would give us an eventual range of 1-11, which is
        perfect.</para>
      </sect2>

      <sect2 id="ByBucketing">
        <title>ByBucketing</title>

        <para>ByBucketing is mathematically similar to ByRounding but with
        rather more ease of use. There is a slight performance hit and rather
        less control with the ByBucketing method. Within the ByBucketing
        method you do not specify the scale or the delta, you simply specify
        the number of eventual buckets (or the number of discrete values) that
        you eventually want. It does a pre-pass of the data to compute the
        scale and delta before applying it.</para>
      </sect2>

      <sect2 id="ByTiling">
        <title>ByTiling</title>

        <para>Both of the previous methods divide the banding evenly across
        the range of the original variable. However, while the range has been
        divided evenly, the number of different records within each band could
        vary greatly. In the height example, one would expect a large number
        of children within the 60-72 inch range (rounded values of 3-7) but
        very few in bands 1 or 11.</para>

        <para>An alternative approach is not to band by some absolute range
        but rather to band on the value of a given value relative to all of
        the other records. For example, you may want to end with 10 bands
        where each band has the same number of records in it; and band 10 is
        the top 10% of the population, band nine is the second 10% etc. This
        result is achieved using the ByTiling scheme. Similar to ByBucketing
        you specify the number of bands you eventually want and the system
        will automatically allocate the field values for you.</para>

        <para>Note that ByTiling does require the data to be sorted and so
        will have an NLgN performance profile.</para>
      </sect2>
    </sect1>

    <sect1 id="Docs_Walkthrough">
      <title>Docs Walk-through</title>

      <para><emphasis role="bold">Modules: Docs</emphasis></para>

      <para>The processing of textual data is a unique problem in the field of
      Machine Learning because of the highly unstructured manner in which
      humans write. There are many ways to say the same thing and many ways
      that different statements can look alike. There is an enormous body of
      research that has been performed to determine algorithms for accurately
      and efficiently extracting useful information from data such as
      electronic documents, articles, and transcriptions, all of which rely on
      human speech patterns.</para>

      <para>The Docs module of the ML library is designed to help prepare
      unstructured and semi-structured text to make it more suitable for
      further processing. This includes routines to decompose the text into
      discrete word elements and collating simple statistics on those tokens,
      such as Term Frequency and Inverse Document Frequency. Also included are
      the basic tools to help determine token association strength using
      industry-standard functions such as Support and Confidence.</para>

      <sect2 id="Tokenize">
        <title>Tokenize</title>

        <para>The Tokenize module breaks a set of raw text into its lexical
        elements. From there, it can produce a dictionary of those elements,
        with weighting, as well as perform integer replacement that
        significantly reduces the space overhead needed to process such large
        amounts of data.</para>

        <para>For the purposes of this walk-through we will be using the
        following limited dataset: <programlisting><?dbfo keep-together="always"?>IMPORT ML;

dSentences:=DATASET([
  {1,'David went to the market and bought milk and bread'},
  {2,'John picked up butter on his way home from work.'},
  {3,'Jill craved lemon cookies, so she grabbed some at the convenience store'},
  {4,'Mary needs milk, bread and butter to make breakfast tomorrow morning.'},
  {5,'William\'s lunch included a sandwich on wheat bread and chocolate chip cookies.'}
],ML.Docs.Types.Raw);
</programlisting></para>

        <para>The format of the initial dataset is in the Raw format in
        Docs.Types, which is a simple numeric ID and a string of free text of
        indeterminate length.</para>

        <para>It is important that a unique ID is assigned to each row so that
        we can have references not just for every word, but for every document
        as well.</para>

        <para>In the above dataset we already have assigned these IDs, but if
        your input table does not yet have them, a quick call to
        Tokenize.Enumerate will assign a sequential integer ID to the
        table:<programlisting><?dbfo keep-together="always"?>dSequenced:=ML.Docs.Tokenize.Enumerate(dSentences)</programlisting></para>

        <para>The first step in parsing the text is to run it through the
        Clean function. This is a simple function that standardizes the text
        by performing actions such as removing punctuation, converting all
        letters into capitals, and normalizing some common
        contractions.<programlisting>dCleaned:=ML.Docs.Tokenize.Clean(dSentences);</programlisting></para>

        <para>Once cleaned, the next step is to break out each word as a
        separate entity using the Split function. A word is defined
        intuitively as a series of non-white-space characters surrounded by
        white space.<programlisting>dSplit:=ML.Docs.Tokenize.Split(dCleaned);</programlisting></para>

        <para>The output produced from the Split function is a 3-column table
        in ML.Docs.Types.WordElement format, with the document ID, the ordinal
        position of the word within the text of that document, and the word
        itself.</para>

        <para>In our example, the first few rows of this table will
        be:<programlisting><?dbfo keep-together="always"?>1   1   DAVID
1   2   WENT
1   3   TO
</programlisting></para>

        <para>This opens us up a number of possibilities for processing our
        text. Most, require one further step, which is to derive some
        aggregate information of the words that appear in our corpus of
        documents. We do this using the Lexicon function:<programlisting>dLexicon:=ML.Docs.Tokenize.Lexicon(dSplit)</programlisting></para>

        <para>This function aggregates the data in our dSplit table, grouping
        on word. The resulting dataset contains one row for each word along
        with a unique ID (an integer starting at 1), a total count of the
        number of times the word occurs in the entire corpus, and the number
        of unique documents within which the word exists. The ID assigned to
        the word is inversely proportional to the word frequency, which means
        that the word that appears the most often will be assigned 1, the next
        most common will have 2, and so on.</para>

        <para>When processing very large amounts of text, there is an
        additional function ToO which can be used to reduce the amount of
        resources used during processing: <programlisting>dReplaced:=ML.Docs.Tokenize.ToO(dSplit,dLexicon);</programlisting></para>

        <para>The output from this function will have as many rows as there
        are in dSplit, but instead of seeing the words as they are in the
        text, you will see the word ID that was assigned to it in dLexicon.
        This saves a large amount of memory because the word ID is always
        4-byte integer, while the word is variable length and usually much
        larger. Since the function has access to the aggregate information
        collected by the Lexicon function, this information is also tacked
        back on to the output from ToO so that it is readily available if
        desired.</para>

        <para>From this point, we have the framework for performing numerous
        Natural Language Processing algorithms, such as keyword designation
        and extraction using the TF/IDF method, or even clustering by treating
        each word ID as a dimension in Euclidean space.</para>

        <para>Finally, the function FromO is pretty self-explanatory. This
        simply re-constitutes a table that was produced by the ToO function
        back into the WordElement format.</para>

        <para><programlisting>dReconstituted:=ML.Docs.Tokenize.FromO(dReplaced,dLexicon);</programlisting></para>
      </sect2>

      <sect2 id="CoLocation">
        <title>Co-Location</title>

        <para>The Docs.CoLocation module takes the textual analysis one step
        further than Tokenize. It harvests n-grams rather than just single
        words and enables the user to perform analyses on those n-grams to
        determine significance. The same dataset (dSentences) that was used in
        the walk-through of the Tokenize module above, is also used as the
        starting point for the examples shown below.</para>

        <para>As with Tokenize, the first step in processing the free text for
        Colocation is to map all of the words. This is done by calling the
        Words attribute, which also calls the Tokenize.Clean and
        Tokenize.Split functions respectively: <programlisting>dWords:=ML.Docs.CoLocation.Words(dSentences);</programlisting></para>

        <para>The AllNGrams attribute then harvests every n-gram, from
        unigrams up to the n defined by the user. This produces result in a
        table that contains a row for every unique id/n-gram combination. In
        the following line, we are asking for anything up to a 4-gram. If the
        n parameter is left blank, the default is 3. <programlisting>dAllNGrams:=ML.Docs.CoLocation.AllNGrams(dWords,,4);</programlisting></para>

        <para>Note that the above call has left the second parameter blank.
        The second parameter is a reference to a Lexicon which is used if you
        decide to perform integer replacement on the words prior to
        processing. This is advisable for very large corpuses. In such a case,
        we would have first called the Lexicon function (which exists in
        CoLocation as a pass-through of the same function in Tokenize) and is
        then passed that output as the second parameter:<programlisting>dLexicon:=ML.Docs.CoLocation.Lexicon(dWords);
dAllNGrams:=ML.Docs.CoLocation.AllNGrams(dWords,dLexicon,4);
</programlisting></para>

        <para>Below are calls to the standard metrics that are currently built
        into the CoLocation module. Remember that the call to Words above has
        called Tokenize.Clean, which has converted all characters in the text
        to uppercase:<programlisting><?dbfo keep-together="always"?>// SUPPORT: User passes a SET OF STRING and the output from the ALLNGrams attribute
ML.Docs.CoLocation.Support(['MILK','BREAD','BUTTER'],dAllNGrams);

// CONFIDENCE, LIFT and CONVICTION: User passes in two SETS OF STRING and the AllNGrams 
   output.
// In each case, set 1 and set 2 are read as â1=&gt;2â.  Note that 1=&gt;2 DOES NOT EQUAL 2=&gt;1.
ML.Docs.CoLocation.Confidence(['MILK','BREAD'],['BUTTER'],dAllNGrams);
ML.Docs.CoLocation.Lift(['MILK','BREAD'],['BUTTER'],dAllNGrams);
ML.Docs.CoLocation.Conviction(['MILK','BREAD'],['BUTTER'],dAllNGrams);
</programlisting></para>

        <para>To further distill the data the user may call NGrams. This
        strips the document IDs and groups the table so that there is one row
        per unique n-gram. Included in this output is aggregate information
        including the number of documents in which the item appears, the
        percentage of that compared to the document count, and the Inverse
        Document Frequency (IDF).<programlisting>dNGrams:=Docs.CoLocation.NGrams(dAllNGrams);</programlisting></para>

        <para>With the output from NGrams there are other attributes that can
        be called to further analyze the data.</para>

        <para>Calling SubGrams produces a table of every n-gram where n&gt;1
        along with a comparison of the document frequency of the n-gram to the
        product of the frequencies of all of its constituent unigrams.</para>

        <para>This gives an indication of whether the phrase or its parts may
        be more significant in the context of the corpus.</para>

        <para><programlisting>ML.Docs.CoLocation.SubGrams(dNGrams);</programlisting></para>

        <para>Another measure of significance is SplitCompare. This splits
        every n-gram with n&gt;1 into two rows with two parts which are the
        initial unigram and the remainder, and the final unigram and the
        remainder. The document frequencies of all three items (the full
        n-gram, and the two constituent parts) are then presented side-by-side
        so their relative values can be evaluated. This helps to determine if
        a leading or trailing word carries any weight in the encompassing
        phrase.<programlisting>ML.Docs.CoLocation.SplitCompare(dNGrams);</programlisting></para>

        <para>Once any analysis has been done and the user has phrases of
        significance, they can be re-constituted using a call to
        ShowPhrase:<programlisting>ML.Docs.CoLocation.ShowPhrase(dLexicon,â14 13 4â);  // would return âCHOCOLATE CHIP COOKIESâ</programlisting></para>
      </sect2>
    </sect1>

    <sect1 id="Field_aggregates_walkthrough">
      <title>Field Aggregates Walkthrough</title>

      <para><emphasis role="bold">Modules: FieldAggregates,
      Distribution</emphasis></para>

      <para>The FieldAggregates module exists to provide statistics upon each
      of the fields of a file. The file is passed in to the field aggregates
      module and then various properties of those fields can be queried, for
      example:</para>

      <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
// Generate random data for testing purposes
TestSize := 10000000;
a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
D := b1+b2; // This is the test data
// Pass the test data into the Aggregate Module
Agg := ML.FieldAggregates(D);
Agg.Simple; // Compute some common statistics
</programlisting>This example provides two rows. The ânumberâ column ties the
      result back to the column being passed in. There are columns for
      min-value, max-value, the sum, the number of rows (with values), the
      mean, the variance and the standard deviation. The âsimpleâ attribute is
      a very good one to use on huge data as it is a simple linear
      process.</para>

      <para>The aggregate module is also able to ârank orderâ a set of data;
      the SimpleRanked attribute allocates every value in every field a number
      â the smallest value gets the number 1, then 2 etc. The âSimpleâ
      indicator is to denote that if a value is repeated the attribute will
      just arbitrarily pick which one gets the lower ranking.</para>

      <para>As you might expect there is also a ârankedâ attribute. In the
      case of multiple identical values this will assign every value with the
      same value a rank which is the average value of the ranks of the
      individual items, for example:</para>

      <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
TestSize := 50;
a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
D := b1+b2; // This is the test data
Agg := ML.FieldAggregates(D);
Agg.SimpleRanked;
Agg.Ranked;
</programlisting></para>

      <para>Note that ranking requires the data to be sorted; therefore
      ranking is an âNlgNâ process.</para>

      <para>When examining the results of the âSimpleâ attribute you may be
      surprised that two of the common averages âmedianâ and âmodeâ are
      missing. While the Aggregate module can return those values, they are
      not included in the âSimpleâ attribute because they are NLgN processes
      and we want to keep âSimpleâ as cheap as possible. The median values for
      each column can be obtained using the following:</para>

      <para><programlisting>Agg.Medians;</programlisting>The modes are found
      by using:<programlisting>Agg.Modes;</programlisting></para>

      <para>It is possible that more than one mode will be returned for a
      particular column, if more than one value has an equal count.</para>

      <para>The final group of features provided by the Aggregate module are
      the NTiles and the Buckets. These are closely related but totally
      different which can be confusing.</para>

      <para>The NTiles are closely related to terms like âpercentilesâ,
      âdecilesâ and âquartilesâ, which allow you to grade each score according
      the a âpercentileâ of the population. The name âNâ tile is there because
      you get to pick the number of groups the population is split into. Use
      NTile(4) for quartiles, NTile(10) for deciles and NTile(100) for
      percentiles. NTile(1000) can be used if you want to be able to split
      populations to one tenth of a percent. Every group (or Tile) will have
      the same number of records within it (unless your data has a lot of
      duplicate values because identical values land in the same tile). The
      following example demonstrates the possible use of NTiling.</para>

      <para>Imagine you have a file with people and for each person you have
      two columns (height and weight). NTile that file with a number, such as
      100. Then if the NTile of the Weight is much higher than the NTile of
      the Height, the person might be overweight. Conversely if the NTile of
      the Height is much higher than the Weight then the person might be
      underweight. If the two percentiles are the same then the person is
      ânormalâ.</para>

      <para>NTileRanges returns information about the highest and lowest value
      in every Tile. Suppose you want to answer the question: âwhat are the
      normal SAT scores for someone going to this collegeâ. You can compute
      the NTileRanges(4). Then you can note both the low value of the second
      quartiles and the high value of the third quartile and declare that âthe
      middle 50% of the students attending that college score between X and
      Yâ.</para>

      <para>The following example demonstrates this:</para>

      <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
TestSize := 100;
a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
D := b1+b2; // This is the test data
Agg := ML.FieldAggregates(D);
Agg.NTiles(4);
Agg.NTileRanges(4)
</programlisting>Buckets provide very similar looking results. However buckets
      do NOT attempt to divide the groups so that the population of each group
      is even. Buckets are divided so that the RANGE of each group is even.
      Suppose that you have a field with a MIN of 0 and MAX of 50 and you ask
      for 10 buckets, the first bucket will be 0 to (almost)5, the second 5 to
      (almost) 10 etc. The Buckets attribute assigns each field value to the
      bucket. The BucketRanges returns a table showing the range of each
      bucket and also the number of elements in that bucket. If you wanted to
      plot a histogram of value verses frequency, for example, buckets would
      be the tool to use.</para>

      <para>The final point to mention is that many of the more sophisticated
      measures use the simpler measures and also share other more complex code
      between themselves. If you eventually want two or more of these measures
      for the same data it is better to compute them all at once. The ECL
      optimizer does an excellent job of making sure code is only executed
      once however often it is used. If you are familiar with ECL at a lower
      level, you may wish to look at the graph for the following:
      <programlisting><?dbfo keep-together="always"?>IMPORT ML;
TestSize := 10000000;
a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
D := b1+b2; // This is the test data
Agg := ML.FieldAggregates(D);
Agg.Simple;
Agg.SimpleRanked;
Agg.Ranked;
Agg.Modes;
Agg.Medians;
Agg.NTiles(4);
Agg.NTileRanges(4);
Agg.Buckets(4);
Agg.BucketRanges(4)
</programlisting></para>
    </sect1>

    <sect1 id="Regression_walkthrough">
      <title>Regression Walk-through</title>

      <para><emphasis role="bold">Modules: Regression</emphasis></para>

      <para>The Regression module exists to perform analysis and modeling of
      the relationship between a single dependent variable Y, and one or more
      independent variables Xi (also called predictor or explanatory
      variables). Regression is called âSimpleâ if only one independent
      variable X is used. It is called âMultivariateâ regression when more
      than one independent variable is used.</para>

      <para>The relationship between dependent variable and independent
      variables is expressed as a function whose form has to be specified.
      Regression is called âLinear Regressionâ if the function that defines
      the relationship is linear. For example, a Simple Linear Regression
      model expresses relationship between dependent variable Y and single
      independent variable X as a linear function:</para>

      <para>Y = Î²0 + Î²1X</para>

      <para>This function represents a line with parameters Î²= (Î²0,
      Î²1).</para>

      <sect2 id="Polynomial">
        <title>Polynomial</title>

        <para>The Polynomial regression expresses (ie models) relationship
        between dependent variable Y and a single independent variable X with
        polynomial function of the following form:</para>

        <para>Y = Î²0 + Î²1*LogX+ Î²2*X+ Î²3*X*LogX+ Î²4*X2+ Î²5*X2*LogX+
        Î²6*X3</para>

        <para>with parameters:</para>

        <para>Î²= (Î²0, Î²1, Î²2, Î²3, Î²4, Î²5, Î²6)</para>

        <para>Along with the other ML functions, Polynomial Regression is
        designed to work upon huge datasets; however it can be quite useful
        even on tiny ones. The following dataset captures the time taken for a
        particular ML routine to execute against a particular number of
        records. Polynomial Regression models relationship between the number
        of records (X=Recs), and the execution time (Y=Time) and produces Î²
        values, which represent how much every component of the polynomial
        function, (constant, Log, XLogX, â¦) contributes to the execution
        time.</para>

        <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
R := RECORD
       INTEGER        rid;
       INTEGER        Recs;
       REAL           Time;
END;
d := DATASET([{1,50000,1.00},{2,500000,2.29}, {3,5000000,16.15},
               {4,25000000,80.2},{5,50000000,163},{6,100000000,316},
               {7,10,0.83},{8,1500000,5.63}],R);
ML.ToField(d,flds);
P := ML.Regression.Poly(flds(number=1),flds(number=2));
P.Beta;
P.RSquared
</programlisting>The Polynomial regression is configured by default to
        calculate parameters Î²= (Î²0, Î²1, Î²2, Î²3, Î²4, Î²5, Î²6). This can be
        changed to any number smaller than 6.</para>

        <para>For example, to configure polynomial regression to use the
        polynomial function Y = Î²0 + Î²1*LogX+ Î²2*X to model relationship
        between X and Y, the code above would need to be changed as
        follows:</para>

        <para><programlisting>P := ML.Regression.Poly(flds(number=1),flds(number=2), 2); </programlisting></para>

        <para>The third parameter â2â has to be used to override the default
        value â6â.</para>
      </sect2>

      <sect2 id="OLS">
        <title>OLS</title>

        <para>The Ordinary Least Squares (OLS) regression is a linear
        regression model that calculates parameters Î² using the method of
        least squares to minimize the distance between measured and predicted
        values of the dependent variable Y. The following example,
        demonstrates how it might be used:</para>

        <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;

value_record := RECORD
                unsigned    rid;
                unsigned    age;
                real        height;
  END;

d := DATASET([{1,18,76.1}, {2,19,77}, {3,20,78.1},
            {4,21,78.2}, {5,22,78.8}, {6,23,79.7},
            {7,24,79.9}, {8,25,81.1}, {9,26,81.2},
            {10,27,81.8},{11,28,82.8}, {12,29,83.5}]
            ,value_record);

ML.ToField(d,o);

X := O(Number =1); // Pull out the age
Y := O(Number =2); // Pull out the height
Reg := ML.Regression.OLS(X,Y);
B := Reg.Beta();
B;                         
Reg.RSquared;
Reg.Anova;
</programlisting>In this example we have a dataset that contains 12 records
        with the age in months and a mean height in centimeters for children
        of that age. We use the OLS regression to find parameters Î² of the
        linear function, ie the line that represents (models) the relationship
        between child age and height. Once we get parameters Î², we can use
        them to predict the height of a child whose age is not listed in the
        dataset.</para>

        <para>For example, the mean height of a 30 month old child can be
        predicted using the following formula:</para>

        <para>Height = Î²0 + 30* Î²1.</para>

        <para>How well does this function (ie regression model) fit the data?
        One measure of goodness of fit is the R-squared. The range of
        R-squared is [0,1], and values closer to 1 indicate better fit. For
        Simple Linear regression the R-squared represents a square of
        correlation between X and Y.</para>

        <para>Analysis of Variance (ANOVA) provides information about the
        level of variability within a regression model, and that information
        can be used as a basis for tests of significance. Anova returns a
        table with the following values for the model, error and total, and
        the model statistic called âFâ:</para>

        <itemizedlist>
          <listitem>
            <para>sum of squares (SS)</para>
          </listitem>

          <listitem>
            <para>degrees of freedom (DF)</para>
          </listitem>

          <listitem>
            <para>mean square (MS)</para>
          </listitem>
        </itemizedlist>

        <para>The OLS regression can be configured to calculate parameters Î²
        using either the LU matrix decomposition or Cholesky matrix
        decomposition. Cholesky matrix decomposition is used as a default if
        no parameters are passed into the beta function. This is how the
        example above has been coded. The default matrix decomposition can be
        changed to the LU decomposition as follows: <programlisting>B := Reg.Beta(Reg.MDM.LU);</programlisting></para>
      </sect2>
    </sect1>
  </chapter>

  <chapter id="ML_modules">
    <title>The ML Modules</title>

    <para>Each ML module focuses on a specific type of algorithm and contains
    a number of routines. The functionality of each routine is also
    described.</para>

    <para>Performance statistics are provided for some routines. These were
    carried out on a 10 node cluster and are for comparison purposes
    only.</para>

    <sect1 id="Associations">
      <title>Associations (ML.Associate)</title>

      <para>Use this module to perform frequent pattern matching on the
      underlying data, as follows:</para>

      <informaltable>
        <tgroup cols="2">
          <colspec align="left" colwidth="190pt" />

          <colspec align="left" colwidth="350pt" />

          <thead>
            <row>
              <entry align="left">Routine</entry>

              <entry align="left">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Apriori1,Apriori2,Apriori3</entry>

              <entry>Uses âold schoolâ brute force and speed approach to
              produce patterns of up to 3 items which appear together with a
              particular degree of support.</entry>
            </row>

            <row>
              <entry>AprioriN</entry>

              <entry>Uses ânew schoolâ techniques finding all patterns of up
              to N items, appearing together with a particular degree of
              support.</entry>
            </row>

            <row>
              <entry>EclatN</entry>

              <entry>Uses the âeclatâ technique to construct a result which is
              identical to AprioriN.</entry>
            </row>

            <row>
              <entry>Rules</entry>

              <entry>Uses patterns generated by AprioriN or EclatN to answer
              the question: âgiven a group of M items exists; what is the
              M+1th most likely to beâ.</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para>The following performance statistics of these routines were
      observed using a 10 node cluster:</para>

      <informaltable>
        <?dbfo keep-together="always"?>

        <tgroup cols="4">
          <colspec align="left" colwidth="60pt" />

          <colspec align="left" colwidth="280pt" />

          <colspec align="left" colwidth="60pt" />

          <colspec align="left" colwidth="140pt" />

          <thead>
            <row>
              <entry align="left">Routine</entry>

              <entry align="left">Description</entry>

              <entry align="left">Result</entry>

              <entry align="left">Expected Order</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Apriori1</entry>

              <entry>On 140M words</entry>

              <entry>47 secs</entry>

              <entry>NlgN</entry>
            </row>

            <row>
              <entry>Aprior1</entry>

              <entry>On 197M words</entry>

              <entry>91 secs</entry>

              <entry>NlgN</entry>
            </row>

            <row>
              <entry>Apiori2</entry>

              <entry>On 140M words, producing 2.6K pairs</entry>

              <entry>325 secs</entry>

              <entry>(N/k)^2.MLg(N) where k is the proportion of âbucketsâ the
              average item is in. (Using terms in 5-10% of buckets)</entry>
            </row>

            <row>
              <entry>Apiori2</entry>

              <entry>On 193M words (using .1-&gt;1% buckets) producing 4.4M
              pairs</entry>

              <entry>21 mins</entry>

              <entry></entry>
            </row>

            <row>
              <entry>Apiori2</entry>

              <entry>On 19M words (10% sample), producing 4.1M pairs</entry>

              <entry>2 mins</entry>

              <entry></entry>
            </row>

            <row>
              <entry>Apiori3</entry>

              <entry>On 140M words (terms in 5-10% buckets)</entry>

              <entry>Exploded</entry>

              <entry></entry>
            </row>

            <row>
              <entry>Apiori3</entry>

              <entry>1.9M words (1% sample of .1-1 buckets), (172K possible 3
              groups), 3.6B intermediate results, 22337 eventual
              results</entry>

              <entry>73 mins</entry>

              <entry></entry>
            </row>

            <row>
              <entry>Apiori3</entry>

              <entry>On 1.9M words with new LOOKUP optimization</entry>

              <entry>42 mins</entry>

              <entry></entry>
            </row>

            <row>
              <entry>EE3</entry>

              <entry>On 1.9M words (1% sample of .1-1 buckets), 22337 eventual
              results</entry>

              <entry>3 mins</entry>

              <entry></entry>
            </row>

            <row>
              <entry>EE10</entry>

              <entry>On 1.9M words</entry>

              <entry>Locks</entry>

              <entry></entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para role="nobrk"></para>

      <para role="nobrk"></para>

      <para role="nobrk"></para>

      <para role="nobrk"></para>

      <para role="nobrk"></para>

      <para role="nobrk"></para>

      <para role="nobrk"></para>

      <para role="nobrk"></para>

      <para role="nobrk"></para>

      <para role="nobrk">This example shows how to use the Apriori
      routine:</para>

      <para><programlisting role="nobrk"><?dbfo keep-together="always"?>IMPORT ML;
IMPORT ML.Docs AS Docs;

d11 := DATASET([{'One of the wonderful things about tiggers is tiggers are wonderful 
                 things'},
                {'It is a little scary the drivel that enters ones mind when given the 
                 task of entering random text'},
                {'I almost quoted Oscar Wilde; but I considered that I had gotten a little 
                 too silly already!'},
                {'I would hate to have quoted silly people!'},
                {'Oscar Wilde is often quoted'},
                {'In Hertford, Hereford and Hampshire Hurricanes hardly ever happen'},
                {'It is a far, far better thing that I do, than I have ever done'}],
                {string r});
d00 := DATASET([{'aa bb cc dd ee'},{'bb cc dd ee ff gg hh ii'},{'bb cc dd ee ff gg hh ii'},
                {'dd ee ff'},{'bb dd ee'}],{string r});

d := d11;

d1 := PROJECT(d,TRANSFORM(Docs.Types.Raw,SELF.Txt := LEFT.r));

d2 := Docs.Tokenize.Enumerate(d1);

d3 := Docs.Tokenize.Clean(d2);

d4 := Docs.Tokenize.Split(d3);                                      


lex := Docs.Tokenize.Lexicon(d4);


o1 := Docs.Tokenize.ToO(d4,lex);
o2 := Docs.Trans(O1).WordBag;

lex;
ForAssoc := PROJECT( o2, TRANSFORM(ML.Types.ItemElement,SELF.id := LEFT.id,
SELF.value := LEFT.word ));
ForAssoc;
ML.Associate(ForAssoc,2).Apriori1;
ML.Associate(ForAssoc,2).Apriori2;
ML.Associate(ForAssoc,2).Apriori3;
ML.Associate(ForAssoc,2).AprioriN(40);
</programlisting></para>

      <para></para>
    </sect1>

    <sect1 id="Classify">
      <title>Classify (ML.Classify)</title>

      <para>Use this module to tackle the problem, âcan I predict this
      dependent variable based upon these independent ones?â. The following
      routines are provided:</para>

      <informaltable>
        <?dbfo keep-together="always"?>

        <tgroup cols="2">
          <colspec align="left" colwidth="130pt" />

          <colspec align="left" colwidth="490pt" />

          <thead>
            <row>
              <entry align="left">Routine</entry>

              <entry align="left">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>BuildNaiveBayes</entry>

              <entry>Builds a Bayes model for one or more dependent
              variables.</entry>
            </row>

            <row>
              <entry>NaiveBayes</entry>

              <entry>Executes one or more Bayes models against an underlying
              dataset to compute dependent variables.</entry>
            </row>

            <row>
              <entry>TestNaiveBayes</entry>

              <entry>Generates a module containing four different measures of
              how well the classification models are doing. This calculation
              is based on the Bayes model and set of independent variables
              with outcome data supplied.</entry>
            </row>

            <row>
              <entry>BuildPerceptron</entry>

              <entry>Builds a perceptron for multiple dependent (Boolean)
              variables.</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para></para>
    </sect1>

    <sect1 id="Cluster">
      <title>Cluster (ML.Cluster)</title>

      <para>This module is used to perform the clustering of a collection of
      records containing fields.</para>

      <para>The following routines are provided:</para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="70pt" />

            <colspec align="left" colwidth="430pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>DF</entry>

                <entry>A submodule which is used to perform various distance
                metrics upon two records. Currently, the following are
                provided: <itemizedlist>
                    <listitem>
                      <para>Euclidean</para>
                    </listitem>

                    <listitem>
                      <para>Euclidean Squared</para>
                    </listitem>

                    <listitem>
                      <para>Manhattan</para>
                    </listitem>

                    <listitem>
                      <para>Cosine</para>
                    </listitem>

                    <listitem>
                      <para>Tanimoto</para>
                    </listitem>
                  </itemizedlist>Quick variants exist for both Euclidean and
                Euclidean Squared which are much faster on sparse data
                PROVIDED you are willing to accept no distance, if there are
                no dimensions along which the vectors touch.</entry>
              </row>

              <row>
                <entry>Distances</entry>

                <entry>The engine to actually compute the distance matrix (as
                a matrix).</entry>
              </row>

              <row>
                <entry>Closest</entry>

                <entry>Takes a set of distances and returns the closest
                centroid for each row.</entry>
              </row>

              <row>
                <entry>KMeans</entry>

                <entry>Performs KMeans clustering for a specified number of
                iterations.</entry>
              </row>

              <row>
                <entry>AggloN</entry>

                <entry>Performs Agglomerative (Hierarchical) clustering. The
                results include cluster assignments, remaining distances
                between clusters and even the dentrogram.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <sect2 id="KMeans">
        <title>KMeans</title>

        <para>The KMeans module performs K-Means clustering on a static
        primary data set and a pre-determined set of centroids.</para>

        <para>This is an iterative two-step process. For each iteration, every
        data entity is assigned to the centroid that is closest to it, and
        then the centroid locations are re-assigned to the mean position of
        all the data entities assigned to them.</para>

        <para>The user passes in the two datasets and the maximum number of
        iterations to perform if convergence is not achieved.</para>

        <para>Optionally, the user may also define a convergence threshold
        (default=0.0) and the distance function to be used during calculations
        (default is Euclidean).</para>

        <para>The primary output from the KMeans module is âResult()â, which
        returns the centroid dataset with their new locations. âConvergenceâ
        will indicate the number of iterations performed, which will never be
        greater than the maximum number passed in to the function.</para>

        <para></para>

        <para></para>

        <para></para>

        <para></para>

        <para></para>

        <para></para>

        <para></para>

        <para></para>

        <para>There are also a number other useful return values that enable
        the user to analyze centroid movement. These are included in the
        following example.</para>

        <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;

lMatrix:={UNSIGNED id;REAL x;REAL y;};

// Simple two-dimensional set of numbers between 0-10
dDocumentMatrix:=DATASET([

  {1,2.4639,7.8579},
  {2,0.5573,9.4681},
  {3,4.6054,8.4723},
  {4,1.24,7.3835},
  {5,7.8253,4.8205},
  {6,3.0965,3.4085},
  {7,8.8631,1.4446},
  {8,5.8085,9.1887},
  {9,1.3813,0.515},
  {10,2.7123,9.2429},
  {11,6.786,4.9368},
  {12,9.0227,5.8075},
  {13,8.55,0.074},
  {14,1.7074,3.9685},
  {15,5.7943,3.4692},
  {16,8.3931,8.5849},
  {17,4.7333,5.3947},
  {18,1.069,3.2497},
  {19,9.3669,7.7855},
  {20,2.3341,8.5196}
],lMatrix);

// Arbitrary but non-symmetric centroid starting points
dCentroidMatrix:=DATASET([
  {1,1,1},
  {2,2,2},
  {3,3,3},
  {4,4,4}
],lMatrix);

// Convert the above matrices into the NumericField format
ML.ToField(dDocumentMatrix,dDocuments);
ML.ToField(dCentroidMatrix,dCentroids);

// Set up KMeans with a maximum of 30 iterations and .3 as a convergence threshold
KMeans:=ML.Cluster.KMeans(dDocuments,dCentroids,30,.3);

// The table that contains the results of each iteration
KMeans.Allresults;
// The number of iterations it took to converge
KMeans.Convergence;
// The results of iteration 3
KMeans.Result(3);
// The distance every centroid travelled across each axis from iterations 3 to 5
KMeans.Delta(3,5);
// The total distance the centroids travelled on each axis
KMeans.Delta(0);
// The straight-line distance travelled by each centroid from iterations 3 to 5
KMeans.DistanceDelta(3,5);
// The total straight-line distance each centroid travelled 
KMeans.DistanceDelta(0);
// The distance travelled by each centroid during the last iteration.
KMeans.DistanceDelta();
</programlisting></para>
      </sect2>

      <sect2 id="AggloN_function">
        <title>AggloN</title>

        <para>The AggloN function performs Agglomerative clustering on a
        static document set. This is a bottom-up approach whereby close pairs
        are found and linked, and then treated as a single entity during the
        next iteration. Allowed to run fully, the result will be a single tree
        structure with multiple branches and sub-branches.</para>

        <para>The dataset and the maximum number of iterations are the two
        required parameters for this function. The user may also optionally
        specify the distance formula which defaults to Euclidean. The
        following is an example using the AggloN routine (using the same
        dDocuments constructed in the above example):</para>

        <para><programlisting><?dbfo keep-together="always"?>// Set up Agglomerative clustering with 4 iterations
AggloN:=ML.Cluster.AggloN(dDocuments,4);

// Table with nested sets of numbers delineating the tree after the specified
// number of iterations
AggloN.Dendrogram;
// Table containing the distances between each pair of clusters
AggloN.Distances;
// List of entities and the cluster they are assigned to (the cluster ID will always
// be the lowest ID value of the entities that comprise the cluster)
AggloN.Clusters;
</programlisting></para>
      </sect2>

      <sect2 id="Distances">
        <title>Distances</title>

        <para>This is the engine for calculating the distances between every
        entity in one dataset to every entity in a second dataset, which are
        the two required parameters. If the first dataset is also passed as
        the second one, then a self-join is performed. Otherwise, it is
        assumed that the IDs for the second dataset do not intersect with
        those from the first one. The default formula used for distance
        calculation is Euclidean, but that may be changed to any of the other
        distance functions available in the DF module.</para>

        <para>The code for the Distances function is fairly complex in an
        effort to facilitate both flexibility and efficiency. The user is able
        to calculate distances in three modes:</para>

        <informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="100pt" />

            <colspec align="left" colwidth="510pt" />

            <tbody>
              <row>
                <entry>Dense</entry>

                <entry>All calculations are performed, which for a self-join
                is an N^2 problem.</entry>
              </row>

              <row>
                <entry>Summary Join</entry>

                <entry>Highly efficient but not as accurate, this process only
                calculates distance between entities that share at least one
                dimension.</entry>
              </row>

              <row>
                <entry>Background</entry>

                <entry>Fully accurate and highly efficient, assuming a sparse
                matrix. This process takes advantage of the assumption that
                only a small number of dimensions are shared between any two
                pairs of entities.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable>

        <para>This is fully accurate and highly efficient, assuming a sparse
        matrix. This process takes advantage of the assumption that only a
        small number of dimensions are shared between any two pairs of
        entities.</para>
      </sect2>
    </sect1>

    <sect1 id="Correlations">
      <title>Correlations (ML.Correlate)</title>

      <para>Use this module to calculate the degree of correlation between
      every pair of fields provided, using the following routines:</para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="100pt" />

            <colspec align="left" colwidth="470pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Simple</entry>

                <entry>Pearson and Spearman correlation co-efficients for
                every pair of fields.</entry>
              </row>

              <row>
                <entry>Kendal</entry>

                <entry>Kendalâs Tau for every pair of fields.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>The correlation tables expose multicollinearity issues and enable
      you to look at predicted versus original to expose heteroscedasticity
      issues.</para>
    </sect1>

    <sect1 id="Discretize">
      <title>Discretize (ML.Discretize)</title>

      <para>This module provides a suite of routines which allow a datastream
      with continuous real elements to be turned into a stream with discrete
      (integer) elements. The Discretize module currently supports three
      methods of discretization, ByRounding, ByBucketing and ByTiling. These
      method can be used by hand for reasons of simplicity and control.</para>

      <para>In addition, it is possible to turn them into an instruction
      stream for an 'engine' to execute. Using them in this way allows the
      discretization strategy to be in meta-data. Use the Do definition to
      construct the meta-data fragment, which will then perform the
      discretization. This enables the automatic generation of strategies and
      even iterates over the modeling process with different discretization
      strategies which can be programmatically generated.</para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="100pt" />

            <colspec align="left" colwidth="470pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>ByRounding</entry>

                <entry>Using scale and delta.</entry>
              </row>

              <row>
                <entry>ByBucketing</entry>

                <entry>Split the range evenly and distribute the values
                potentially unevenly.</entry>
              </row>

              <row>
                <entry>ByTiling</entry>

                <entry>Split the values evenly and have an uneven
                range.</entry>
              </row>

              <row>
                <entry>Do</entry>

                <entry>Constructs a meta-data fragment which will then perform
                the discretization.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>The following is an example of the use of the Naive Bayes
      routine:</para>

      <para><programlisting><?dbfo keep-together="always"?>import ml;

value_record := RECORD
        unsigned rid;
        real height;
        real weight;
        real age;
        integer1 species;
        integer1 gender; // 0 = unknown, 1 = male, 2 = female
END;

d := dataset([{1,5*12+7,156*16,43,1,1},
              {2,5*12+7,128*16,31,1,2},
              {3,5*12+9,135*16,15,1,1},
              {4,5*12+7,145*16,14,1,1},
              {5,5*12-2,80*16,9,1,1},
              {6,4*12+8,72*16,8,1,1},
              {7,8,32,2.5,2,2},
              {8,6.5,28,2,2,2},
              {9,6.5,28,2,2,2},
              {10,6.5,21,2,2,1},
              {11,4,15,1,2,0},
              {12,3,10.5,1,2,0},
              {13,2.5,3,0.8,2,0},
              {14,1,1,0.4,2,0}
              ]
              ,value_record);
// Turn into regular NumericField file (with continuous variables)
ml.ToField(d,o);

// Hand-code the discretization of some of the variables
disc := ML.Discretize.ByBucketing(o(Number IN [2,3]),4)+ML.Discretize.ByTiling(o(Number IN
[1]),6)+ML.Discretize.ByRounding(o(Number=4));

// Create instructions to be executed
inst := 
ML.Discretize.i_ByBucketing([2,3],4)+ML.Discretize.i_ByTiling([1],6)+
ML.Discretize.i_ByRounding([4,5]);

// Execute the instructions
done := ML.Discretize.Do(o,inst);

//m1 := ML.Classify.BuildPerceptron(done(Number&lt;=3),done(Number&gt;=4));
//m1

m1 := ML.Classify.BuildNaiveBayes(done(Number&lt;=3),done(Number&gt;=4));
m1;

Test := ML.Classify.TestNaiveBayes(done(Number&lt;=3),done(Number&gt;=4),m1);
Test.Raw;
Test.CrossAssignments;
Test.PrecisionByClass;
Test.Headline;
</programlisting></para>

      <para>The following is an example of the use of the discretize
      routines:</para>

      <para><programlisting><?dbfo keep-together="always"?>import ml;
value_record := RECORD
                unsigned rid;
                real height;
                real weight;
                real age;
                integer1 species;
END;
d := dataset([{1,5*12+7,156*16,43,1},
              {2,5*12+7,128*16,31,1},
              {3,5*12+9,135*16,15,1},
              {4,5*12+7,145*16,14,1},
              {5,5*12-2,80*16,9,1},
              {6,4*12+8,72*16,8,1},
              {7,8,32,2.5,2},
              {8,6.5,28,2,2},
              {9,6.5,28,2,2},
              {10,6.5,21,2,2},
              {11,4,15,1,2},
              {12,3,10.5,1,2},
              {13,2.5,3,0.8,2},
              {14,1,1,0.4,2}             
              ]
              value_record);
// Turn into regular NumericField file (with continuous variables)
ml.macPivot(d,o);

// Hand-code the discretization of some of the variables
disc := ML.Discretize.ByBucketing(o(Number = 3),4)+ML.Discretize.ByTiling(o
(Number IN [1,2]),4)+ML.Discretize.ByRounding(o(Number=4));
disc;

// Create instructions to be executed
inst := ML.Discretize.i_ByBucketing([3],4)+ML.Discretize.i_ByTiling([1,2],4)
+ML.Discretize.i_ByRounding([4]);

// Execute the instructions
done := ML.Discretize.Do(o,inst);
done;
</programlisting></para>
    </sect1>

    <sect1 id="Distribution">
      <title>Distribution (ML.Distribution)</title>

      <para>The distribution module exists to provide code to generate
      distribution tables and ârandomâ data according to a particular
      distribution.</para>

      <para>Each distribution is a âmoduleâ that takes a collection of
      parameters and then implements a common interface. Other than the
      ânaturalâ mathematics of each distribution the implementation adds the
      notion of ranges (or NRanges). Essentially this means that the codomain
      (range) of the distribution is split into NRanges; this can be thought
      of as a degree of granularity. For discrete distributions this is
      naturally the number of results that can be produced. For continuous
      distributions you should think of the distribution curve as being
      approximated by NRanges straight lines. The maximum granularity
      currently supported in 1M ranges (after that the numeric pain of
      retaining precision is too nasty).</para>

      <para>The interface to the distribution provides:</para>

      <para><programlisting><?dbfo keep-together="always"?>  EXPORT t_FieldReal Density(t_FieldReal RH)
// The probability density function at point RH

EXPORT t_FieldReal Cumulative(t_FieldReal RH)
// The cumulative probability function from â infinity[1] up to RH

  EXPORT DensityV() 
// A vector providing the probability density function at each range point â 
   this is approximately equal to the âdistribution tablesâ that might be published 
   in various books

EXPORT CumulativeV()
// A vector providing the cumulative probability density function at each range point 
   â again roughly equal to âcumulative distribution tablesâ as published in the back of 
   statistics books

EXPORT Ntile(Pcnt
//provides the value from the underlying domain that corresponds to the given percentile. 
  Thus .Ntile(99) gives the value beneath which 99% of all should observations will fall.
</programlisting></para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="150pt" />

            <colspec align="left" colwidth="470pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Uniform (Low, High, NRanges.)</entry>

                <entry>Specifies that any (continuous) value between Low and
                High is equally likely to occur.</entry>
              </row>

              <row>
                <entry>StudentT (degrees-of-freedon, NRanges)</entry>

                <entry>Specifies the degrees of freedom for a Student-T
                distribution. Note this module also exports InvDensity to
                provide the âtâ value that gives a particular density value
                (can be useful as the âtailâ of a t distribution is often
                interesting).</entry>
              </row>

              <row>
                <entry>Normal (Mean, Standard Deviation, NRanges)</entry>

                <entry>Implements a normal distribution (bell curve), which
                shows mean âmeanâ and standard deviation as specified,
                approximate by NRanges straight lines.</entry>
              </row>

              <row>
                <entry>Exponential (Lamda, NRanges))</entry>

                <entry>Implements the exponential (sometimes called negative
                exponential) distribution.</entry>
              </row>

              <row>
                <entry>Binomial (p, NRanges)</entry>

                <entry>Gives the distribution showing the chances of getting
                âkâ successful events in Nranges-1 trials where the chances of
                success in one trial is âpâ.</entry>
              </row>

              <row>
                <entry>NegBinomial (p, failures, NRanges)</entry>

                <entry>Gives the distribution showing the chances of getting
                âkâ successful events before âfailuresâ number of failures
                occurs (maximum total trials = nranges). The geometric
                distribution can be obtained by setting failures = 1.</entry>
              </row>

              <row>
                <entry>Poisson (Lamda, NRanges)</entry>

                <entry>For a poisson distribution the mean and variance are
                both provided by Lamda. It is a discrete distribution (will
                only produce integral values). Thus if NRanges is (say) 100
                then the probability function for Poisson will be computed for
                values of 0 to NRanges-1.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>Most people do not really encounter StudentT as a distribution,
      rather they encounter the t-test. You can perform a t-test using the t
      distribution using the NTile capability. Thus the value for a
      single-tailed t-test with 3 degrees of freedom at the 99% confidence
      level can be obtained using:<programlisting><?dbfo keep-together="always"?>a := ML.Distribution.StudentT(3,10000);
a.NTile(99); // Single tail
a.NTile(99.5); // Double tail
</programlisting></para>

      <para>Note that the Cauchy distribution can be obtained by setting v =
      1.</para>

      <para>This module also exports:</para>

      <para><programlisting>GenData(NRecords,Distribution,FieldNumber)</programlisting>This
      allows N records to be generated each with a given field-number and with
      random values distributed according to the specified distribution. If a
      âsingle streamâ of random numbers is required then FieldNumber may be
      set to 1. It is provided to allow ârandom recordsâ with multiple fields
      to be produced.</para>

      <para>For example:<programlisting><?dbfo keep-together="always"?>IMPORT * FROM ML;
//a := ML.Distribution.Normal(4,5,10000);
a := ML.Distribution.Poisson(40,100);
//a := ML.Distribution.Uniform(0,100,10000);
a.Cumulative(5);
choosen(a.DensityV(),1000);
choosen(a.CumulativeV(),1000);
b := ML.Distribution.GenData(200000,a);
ave(b,value);
variance(b,value)
</programlisting></para>

      <para>The following performance timings were done generating 3 fields,
      two normal and one poisson:</para>

      <informaltable>
        <?dbfo keep-together="always"?>

        <tgroup cols="2">
          <colspec align="center" colwidth="130pt" />

          <colspec align="center" colwidth="130pt" />

          <thead>
            <row>
              <entry align="center">Records</entry>

              <entry align="center">Time (secs)</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>50K</entry>

              <entry>1.00</entry>
            </row>

            <row>
              <entry>500K</entry>

              <entry>2.29</entry>
            </row>

            <row>
              <entry>5M</entry>

              <entry>16.15</entry>
            </row>

            <row>
              <entry>25M</entry>

              <entry>80.2</entry>
            </row>

            <row>
              <entry>50M</entry>

              <entry>163</entry>
            </row>

            <row>
              <entry>100M</entry>

              <entry>316</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para></para>
    </sect1>

    <sect1 id="FieldAggregates_Module">
      <title>FieldAggregates (ML.FieldAggregates)</title>

      <para>This module works on the field elements provided. It performs the
      tasks shown below for ALL the fields at once:</para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="130pt" />

            <colspec align="left" colwidth="470pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Simple stats</entry>

                <entry>Mean, Variance, Standard Deviation, Max, Min, Count,
                Sums, etc</entry>
              </row>

              <row>
                <entry>Medians</entry>

                <entry>Provides the median elements.</entry>
              </row>

              <row>
                <entry>Modes</entry>

                <entry>Provides the modal value(s) of each field.</entry>
              </row>

              <row>
                <entry>Cardinality</entry>

                <entry>Provides the cardinality of each field.</entry>
              </row>

              <row>
                <entry>Buckets/Bucket Ranges</entry>

                <entry>Divides the domain of each field evenly and then counts
                the number of elements falling into each range. Can be used to
                graphically plot the distribution of a field</entry>
              </row>

              <row>
                <entry>SimpleRanked</entry>

                <entry>Lists the ranking of the elements from the smallest to
                the largest.</entry>
              </row>

              <row>
                <entry>Ranked</entry>

                <entry>Adjusts the simple ranking to allow for repeated
                elements (each repeated element gets a rank which is the mean
                of the ranks provided to each element individually)</entry>
              </row>

              <row>
                <entry>NTiles/NTileRanges</entry>

                <entry>Think of this as giving the percentile rank of each
                element, except you get to pick if it is percentiles (N=100)
                or some other gradation</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>
    </sect1>

    <sect1 id="Regression">
      <title>Regression (ML.Regression)</title>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="80pt" />

            <colspec align="left" colwidth="510pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>OLS</entry>

                <entry>Takes a set of independent and dependent variables and
                exports a module which publishes an ordinary least squares
                linear regression of the independent variables to produce the
                dependent ones (it can perform multiple regressions at
                once).</entry>
              </row>

              <row>
                <entry>Poly</entry>

                <entry>Performs a polynomial regression for a single
                independent variable with a single dependent target.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>The OLS routine can also return R^2 and Anova tables based upon
      the regression.</para>

      <para>Currently, Poly handles polynomials up to X^3 and includes
      logarithms. Along with the other ML functions Poly is designed to work
      upon huge datasets; however it can be quite useful even on tiny ones.
      The following dataset captures the time taken for a particular ML
      routine to execute against a particular number of records. It then
      produces a table showing the expected running time for any number of
      records that are entered:<programlisting>IMPORT ML;

R := RECORD
                INTEGER rid;
  INTEGER Recs;
                REAL Time;
                END;      
d := DATASET([{1,50000,1.00},{2,500000,2.29},         {3,5000000,16.15},{4,25000000,80.2},
                                                      {5,50000000,163},{6,100000000,316},
                                                      {7,10,0.83},{8,1500000,5.63}],R);

ML.ToField(d,flds);

P := ML.Regression.Poly(flds(number=1),flds(number=2),4);
P.Beta;
P.RSquared
</programlisting></para>

      <para>The R squared is a measure of goodness of fit.</para>
    </sect1>
  </chapter>

  <chapter id="ML_with_documents">
    <title>Using ML with documents (ML.Docs)</title>

    <para>The ML.Docs module provides a number of routines which can be used
    to pre-process text and make it more suitable for further processing. The
    following routines are provided:<informaltable>
        <tgroup cols="3">
          <colspec align="left" colwidth="80pt" />

          <colspec align="left" colwidth="90pt" />

          <colspec align="left" colwidth="430pt" />

          <thead>
            <row>
              <entry align="left">Routine</entry>

              <entry align="left">Sub-Routine</entry>

              <entry align="left">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Tokenize</entry>

              <entry></entry>

              <entry>Routines which turn raw text into a clean processing
              format.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Enumerate</entry>

              <entry>Applies record numbers to the text for later
              tracking.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Clean</entry>

              <entry>Removes lots of nasty punctuation and some other things
              such as possessives.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Split</entry>

              <entry>Turns the document into a token (or word) stream.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Lexicon</entry>

              <entry>Constructs a dictionary (with various statistics) on the
              underlying documents.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>ToO/FromO</entry>

              <entry>Uses the lexicon to turn the word stream to and from an
              optimized token processing format.</entry>
            </row>

            <row>
              <entry>Trans</entry>

              <entry></entry>

              <entry>Performs various data transformations on an optimized
              document stream.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>WordBag</entry>

              <entry>Turns every document into a wordbag, by removing (and
              counting) multiple occurrences of a word within a
              document.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>WordsCounted</entry>

              <entry>Annotates every word in a document with the total number
              of times that word occurs in the document and distributes tfdi
              information for further (faster) processing.</entry>
            </row>

            <row>
              <entry>CoLocation</entry>

              <entry></entry>

              <entry>Functions related to the co-occurrence of NGrams and
              their constituents.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>AllNGrams</entry>

              <entry>A breakdown of all n-grams within a set of raw document
              text, where n is specified by the user.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>NGrams</entry>

              <entry>Aggregate information for every unique n-gram found
              within the document corpus.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Support</entry>

              <entry>The ratio of the number of documents which contain all of
              the n-grams in the parameter set compared to the corpus
              count.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Confidence</entry>

              <entry>The ratio of the number of documents which contain all of
              the n-grams in both parameter sets, compared to the number of
              documents which only contain the n-grams in the first
              set.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Lift</entry>

              <entry>The ratio of observed support of two sets of n-grams
              compared to the support if the n-grams were independent.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Conviction</entry>

              <entry>The ratio of how often one set of n-grams exists without
              a second set given that the two are independent.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>SubGrams</entry>

              <entry>Compares the product of the document frequencies of all
              constituent unigrams to the n-gram they comprise.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>SplitCompare</entry>

              <entry>Compares the document frequency of every n-gram to the
              individual frequencies of a member unigram and the remaining
              n-1-gram.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>ShowPhrase</entry>

              <entry>If lexical substitution was using in constructing the
              AllNGrams dataset, this function will re-constitute the text
              associated with their corresponding numbers within an
              n-gram.</entry>
            </row>

            <row>
              <entry>PorterStem</entry>

              <entry></entry>

              <entry>Standard Porter stemmer in ECL.</entry>
            </row>

            <row>
              <entry>PorterStemC</entry>

              <entry></entry>

              <entry>Standard Porter stemmer in C (faster).</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable></para>

    <sect1 id="Doc_module_usage">
      <title>Typical usage of the Docs module</title>

      <para>The following code demonstrates how the docs module may be used
      and show the result at each stage:</para>

      <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
IMPORT ML.Docs AS Docs;

d := DATASET([{'One of the wonderful things about tiggers is tiggers are wonderful things'},
                                  {'It is a little scary the drivel that enters ones mind 
                                    when given the task of entering random text'},
                                  {'I almost quoted Oscar Wilde; but I considered that I 
                                    had gotten a little too silly already!'},
                                  {'In Hertford, Hereford and Hampshire Hurricanes hardly 
                                    ever happen'},
                                  {'It is a far, far better thing that I do, than I have 
                                    ever done'}],{string r});

d1 := PROJECT(d,TRANSFORM(Docs.Types.Raw,SELF.Txt := LEFT.r));

d1;

d2 := Docs.Tokenize.Enumerate(d1);

d2;

d3 := Docs.Tokenize.Clean(d2);

d3;

d4 := Docs.Tokenize.Split(d3); 

d4;

lex := Docs.Tokenize.Lexicon(d4);

lex;

o1 := Docs.Tokenize.ToO(d4,lex);
o1;

Docs.Trans(o1).WordBag;
Docs.Trans(o1).WordsCounted;

o2 := Docs.Tokenize.FromO(o1,lex);
o2;
</programlisting></para>
    </sect1>

    <sect1 id="Performance_statistics">
      <title>Performance Statistics</title>

      <para>The following performance statistics of these routines were
      observed using a 10 node cluster:</para>

      <informaltable>
        <?dbfo keep-together="always"?>

        <tgroup cols="4">
          <colspec align="left" colwidth="120pt" />

          <colspec align="left" colwidth="280pt" />

          <colspec align="left" colwidth="80pt" />

          <colspec align="left" colwidth="120pt" />

          <thead>
            <row>
              <entry align="left">Routine</entry>

              <entry align="left">Description</entry>

              <entry align="left">Result</entry>

              <entry align="left">Expected Order</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Clean and Split</entry>

              <entry>22m documents producing 1.5B words.</entry>

              <entry>60 minutes</entry>

              <entry>Linear</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>1.5B words producing 6.4M entries.</entry>

              <entry>40 minutes</entry>

              <entry>NlgN</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>Creating 'working' entries from 1.5B words and the full
              6.4m entry lexicon.</entry>

              <entry>46 minutes</entry>

              <entry>NLgN</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>Creating âworkingâ entries from 1.5B words and the
              âkeywordâ lexicon, produces 140M words.</entry>

              <entry>37 minutes</entry>

              <entry>NLgN</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>Creating âworkingâ entries from 1.5B words and the
              âkeyword MkIIâ lexicon, produces 240M words.</entry>

              <entry>40 minutes</entry>

              <entry>NLgN</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>Creating âworkingâ entries from 1.5B words and the
              âkeyword MkIIâ lexicon, produces 240M words, using the âsmall
              lexiconâ feature.</entry>

              <entry>7 minutes</entry>

              <entry>Linear</entry>
            </row>

            <row>
              <entry>Wordbag</entry>

              <entry>Create WordBags from 140M words.</entry>

              <entry>114 seconds</entry>

              <entry>NlgN</entry>
            </row>

            <row>
              <entry>Wordbag</entry>

              <entry>Create WordBags from 240M-&gt;193M words.</entry>

              <entry>297 seconds</entry>

              <entry>NlgN</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>
    </sect1>
  </chapter>

  <chapter id="ML_implementation">
    <title>Useful routines for ML implementation</title>

    <para>The following modules are provided to help with ML
    implementation:</para>

    <itemizedlist>
      <listitem>
        <para>The Utility module</para>
      </listitem>
    </itemizedlist>

    <para><itemizedlist>
        <listitem>
          <para>The Matrix module</para>
        </listitem>
      </itemizedlist></para>

    <sect1 id="Utility">
      <title>Utility</title>

      <para>This module includes the following:</para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="250pt" />

            <colspec align="left" colwidth="350pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>ML.ToFieldElement/FromFieldElement</entry>

                <entry>Translates in and out of the core field-element
                datamodel.</entry>
              </row>

              <row>
                <entry>ML.Types.ToMatrix/FromMatrix</entry>

                <entry>Translates from field elements to and from
                matrices.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>
    </sect1>

    <sect1 id="Matrix_Library">
      <title>The Matrix Library (Mat)</title>

      <para>This library is in addition to and subservient to the ML library
      modules.</para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="110pt" />

            <colspec align="left" colwidth="490pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Add</entry>

                <entry>Add two matrices.</entry>
              </row>

              <row>
                <entry>Choleski</entry>

                <entry>Decomposition.</entry>
              </row>

              <row>
                <entry>Det</entry>

                <entry>Determinant of a matrix.</entry>
              </row>

              <row>
                <entry>Each</entry>

                <entry>Some âelement by elementâ processing steps.</entry>
              </row>

              <row>
                <entry>Eq</entry>

                <entry>Are two matrices equal.</entry>
              </row>

              <row>
                <entry>Has</entry>

                <entry>Various matrix properties.</entry>
              </row>

              <row>
                <entry>Identity</entry>

                <entry>Construct an Identity Matrix.</entry>
              </row>

              <row>
                <entry>InsertColumn</entry>

                <entry>Add a column to a matrix.</entry>
              </row>

              <row>
                <entry>Inv</entry>

                <entry>Invert a matrix.</entry>
              </row>

              <row>
                <entry>Is</entry>

                <entry>Boolean tests for certain matrix types (Identity, Zero,
                Diagonal, Triangular etc).</entry>
              </row>

              <row>
                <entry>LU</entry>

                <entry>LU decomposition of a matrix.</entry>
              </row>

              <row>
                <entry>MU</entry>

                <entry>Matrix Universe. A number of routines to allow multiple
                matrices to exist within the same âfileâ (or dataflow). Useful
                for iterating around loops etc.</entry>
              </row>

              <row>
                <entry>Mul</entry>

                <entry>Multiply two matrices.</entry>
              </row>

              <row>
                <entry>Pow</entry>

                <entry>Multiplies a matrix by itself N-1 * (and demoâs
                MU).</entry>
              </row>

              <row>
                <entry>RoundDelta</entry>

                <entry>Round all the elements of a matrix if they are within
                delta of an integer.</entry>
              </row>

              <row>
                <entry>Scale</entry>

                <entry>Multiply a matrix by a constant.</entry>
              </row>

              <row>
                <entry>Sub</entry>

                <entry>Subtract one matrix from another.</entry>
              </row>

              <row>
                <entry>Substitute</entry>

                <entry>Construct a matrix which is all the elements of the
                right + any elements from the left which are not in the
                right.</entry>
              </row>

              <row>
                <entry>Thin</entry>

                <entry>Make sure a matrix is fully sparse.</entry>
              </row>

              <row>
                <entry>Trans</entry>

                <entry>Construct the transpose of a matrix.</entry>
              </row>

              <row>
                <entry>Vec</entry>

                <entry>A vector library, to create vectors from matrices and
                assign vectors into matrices.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>The following examples demonstrate some of these routines:</para>

      <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
IMPORT ML.Mat AS Mat;
d := dataset([{1,1,1.0},{1,2,2.0},{2,1,3.0},{2,2,4.0}],Mat.Types.Element);

Mat.Sub( Mat.Scale(d,10.0), d );
Mat.Mul(d,d);
Mat.Trans(d);
</programlisting></para>
    </sect1>
  </chapter>
</book>
